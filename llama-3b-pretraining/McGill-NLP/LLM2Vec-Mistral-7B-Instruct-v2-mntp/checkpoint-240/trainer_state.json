{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.09159212880143112,
  "eval_steps": 500,
  "global_step": 240,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00038163387000596303,
      "grad_norm": 0.625,
      "learning_rate": 2.999618320610687e-05,
      "loss": 0.3496,
      "step": 1
    },
    {
      "epoch": 0.0007632677400119261,
      "grad_norm": 0.54296875,
      "learning_rate": 2.9992366412213742e-05,
      "loss": 0.3094,
      "step": 2
    },
    {
      "epoch": 0.001144901610017889,
      "grad_norm": 0.52734375,
      "learning_rate": 2.998854961832061e-05,
      "loss": 0.3432,
      "step": 3
    },
    {
      "epoch": 0.0015265354800238521,
      "grad_norm": 0.5625,
      "learning_rate": 2.9984732824427483e-05,
      "loss": 0.3068,
      "step": 4
    },
    {
      "epoch": 0.0019081693500298152,
      "grad_norm": 0.71484375,
      "learning_rate": 2.9980916030534352e-05,
      "loss": 0.2864,
      "step": 5
    },
    {
      "epoch": 0.002289803220035778,
      "grad_norm": 0.8984375,
      "learning_rate": 2.997709923664122e-05,
      "loss": 0.2631,
      "step": 6
    },
    {
      "epoch": 0.002671437090041741,
      "grad_norm": 0.890625,
      "learning_rate": 2.9973282442748093e-05,
      "loss": 0.2282,
      "step": 7
    },
    {
      "epoch": 0.0030530709600477043,
      "grad_norm": 0.7578125,
      "learning_rate": 2.9969465648854962e-05,
      "loss": 0.1776,
      "step": 8
    },
    {
      "epoch": 0.003434704830053667,
      "grad_norm": 0.91015625,
      "learning_rate": 2.996564885496183e-05,
      "loss": 0.1641,
      "step": 9
    },
    {
      "epoch": 0.0038163387000596303,
      "grad_norm": 1.0078125,
      "learning_rate": 2.9961832061068703e-05,
      "loss": 0.161,
      "step": 10
    },
    {
      "epoch": 0.004197972570065593,
      "grad_norm": 1.1015625,
      "learning_rate": 2.9958015267175572e-05,
      "loss": 0.1309,
      "step": 11
    },
    {
      "epoch": 0.004579606440071556,
      "grad_norm": 1.0546875,
      "learning_rate": 2.9954198473282444e-05,
      "loss": 0.1161,
      "step": 12
    },
    {
      "epoch": 0.00496124031007752,
      "grad_norm": 1.2265625,
      "learning_rate": 2.9950381679389313e-05,
      "loss": 0.1175,
      "step": 13
    },
    {
      "epoch": 0.005342874180083482,
      "grad_norm": 1.3984375,
      "learning_rate": 2.9946564885496182e-05,
      "loss": 0.1111,
      "step": 14
    },
    {
      "epoch": 0.005724508050089445,
      "grad_norm": 1.015625,
      "learning_rate": 2.9942748091603054e-05,
      "loss": 0.0807,
      "step": 15
    },
    {
      "epoch": 0.0061061419200954085,
      "grad_norm": 0.8046875,
      "learning_rate": 2.9938931297709923e-05,
      "loss": 0.0786,
      "step": 16
    },
    {
      "epoch": 0.006487775790101372,
      "grad_norm": 0.98046875,
      "learning_rate": 2.9935114503816795e-05,
      "loss": 0.0889,
      "step": 17
    },
    {
      "epoch": 0.006869409660107334,
      "grad_norm": 0.80859375,
      "learning_rate": 2.9931297709923664e-05,
      "loss": 0.0654,
      "step": 18
    },
    {
      "epoch": 0.007251043530113297,
      "grad_norm": 1.0234375,
      "learning_rate": 2.9927480916030533e-05,
      "loss": 0.0811,
      "step": 19
    },
    {
      "epoch": 0.007632677400119261,
      "grad_norm": 0.76953125,
      "learning_rate": 2.9923664122137405e-05,
      "loss": 0.0546,
      "step": 20
    },
    {
      "epoch": 0.008014311270125224,
      "grad_norm": 0.69140625,
      "learning_rate": 2.9919847328244274e-05,
      "loss": 0.0466,
      "step": 21
    },
    {
      "epoch": 0.008395945140131186,
      "grad_norm": 0.73046875,
      "learning_rate": 2.9916030534351143e-05,
      "loss": 0.0639,
      "step": 22
    },
    {
      "epoch": 0.00877757901013715,
      "grad_norm": 0.88671875,
      "learning_rate": 2.9912213740458015e-05,
      "loss": 0.0572,
      "step": 23
    },
    {
      "epoch": 0.009159212880143113,
      "grad_norm": 0.84765625,
      "learning_rate": 2.9908396946564884e-05,
      "loss": 0.0742,
      "step": 24
    },
    {
      "epoch": 0.009540846750149075,
      "grad_norm": 0.828125,
      "learning_rate": 2.9904580152671756e-05,
      "loss": 0.0716,
      "step": 25
    },
    {
      "epoch": 0.00992248062015504,
      "grad_norm": 0.6875,
      "learning_rate": 2.9900763358778625e-05,
      "loss": 0.0367,
      "step": 26
    },
    {
      "epoch": 0.010304114490161002,
      "grad_norm": 0.95703125,
      "learning_rate": 2.9896946564885494e-05,
      "loss": 0.0673,
      "step": 27
    },
    {
      "epoch": 0.010685748360166964,
      "grad_norm": 0.57421875,
      "learning_rate": 2.9893129770992366e-05,
      "loss": 0.0388,
      "step": 28
    },
    {
      "epoch": 0.011067382230172928,
      "grad_norm": 0.6484375,
      "learning_rate": 2.9889312977099235e-05,
      "loss": 0.0655,
      "step": 29
    },
    {
      "epoch": 0.01144901610017889,
      "grad_norm": 0.5703125,
      "learning_rate": 2.9885496183206107e-05,
      "loss": 0.0362,
      "step": 30
    },
    {
      "epoch": 0.011830649970184855,
      "grad_norm": 0.5703125,
      "learning_rate": 2.9881679389312976e-05,
      "loss": 0.041,
      "step": 31
    },
    {
      "epoch": 0.012212283840190817,
      "grad_norm": 0.73828125,
      "learning_rate": 2.987786259541985e-05,
      "loss": 0.055,
      "step": 32
    },
    {
      "epoch": 0.01259391771019678,
      "grad_norm": 0.67578125,
      "learning_rate": 2.987404580152672e-05,
      "loss": 0.0495,
      "step": 33
    },
    {
      "epoch": 0.012975551580202744,
      "grad_norm": 0.6484375,
      "learning_rate": 2.987022900763359e-05,
      "loss": 0.053,
      "step": 34
    },
    {
      "epoch": 0.013357185450208706,
      "grad_norm": 0.6484375,
      "learning_rate": 2.986641221374046e-05,
      "loss": 0.0563,
      "step": 35
    },
    {
      "epoch": 0.013738819320214668,
      "grad_norm": 0.82421875,
      "learning_rate": 2.986259541984733e-05,
      "loss": 0.0352,
      "step": 36
    },
    {
      "epoch": 0.014120453190220632,
      "grad_norm": 0.41015625,
      "learning_rate": 2.98587786259542e-05,
      "loss": 0.0345,
      "step": 37
    },
    {
      "epoch": 0.014502087060226595,
      "grad_norm": 0.5859375,
      "learning_rate": 2.9854961832061072e-05,
      "loss": 0.0473,
      "step": 38
    },
    {
      "epoch": 0.014883720930232559,
      "grad_norm": 0.470703125,
      "learning_rate": 2.985114503816794e-05,
      "loss": 0.0322,
      "step": 39
    },
    {
      "epoch": 0.015265354800238521,
      "grad_norm": 0.58203125,
      "learning_rate": 2.984732824427481e-05,
      "loss": 0.0342,
      "step": 40
    },
    {
      "epoch": 0.015646988670244485,
      "grad_norm": 0.5703125,
      "learning_rate": 2.9843511450381682e-05,
      "loss": 0.0348,
      "step": 41
    },
    {
      "epoch": 0.016028622540250448,
      "grad_norm": 0.53515625,
      "learning_rate": 2.983969465648855e-05,
      "loss": 0.0412,
      "step": 42
    },
    {
      "epoch": 0.01641025641025641,
      "grad_norm": 0.484375,
      "learning_rate": 2.9835877862595423e-05,
      "loss": 0.0417,
      "step": 43
    },
    {
      "epoch": 0.016791890280262373,
      "grad_norm": 0.310546875,
      "learning_rate": 2.9832061068702292e-05,
      "loss": 0.0186,
      "step": 44
    },
    {
      "epoch": 0.017173524150268335,
      "grad_norm": 0.45703125,
      "learning_rate": 2.982824427480916e-05,
      "loss": 0.0428,
      "step": 45
    },
    {
      "epoch": 0.0175551580202743,
      "grad_norm": 0.404296875,
      "learning_rate": 2.9824427480916033e-05,
      "loss": 0.029,
      "step": 46
    },
    {
      "epoch": 0.017936791890280263,
      "grad_norm": 0.51171875,
      "learning_rate": 2.9820610687022902e-05,
      "loss": 0.0349,
      "step": 47
    },
    {
      "epoch": 0.018318425760286226,
      "grad_norm": 0.57421875,
      "learning_rate": 2.981679389312977e-05,
      "loss": 0.0455,
      "step": 48
    },
    {
      "epoch": 0.018700059630292188,
      "grad_norm": 0.423828125,
      "learning_rate": 2.9812977099236643e-05,
      "loss": 0.0342,
      "step": 49
    },
    {
      "epoch": 0.01908169350029815,
      "grad_norm": 0.439453125,
      "learning_rate": 2.9809160305343512e-05,
      "loss": 0.0301,
      "step": 50
    },
    {
      "epoch": 0.019463327370304113,
      "grad_norm": 0.427734375,
      "learning_rate": 2.9805343511450384e-05,
      "loss": 0.0399,
      "step": 51
    },
    {
      "epoch": 0.01984496124031008,
      "grad_norm": 0.546875,
      "learning_rate": 2.9801526717557253e-05,
      "loss": 0.0548,
      "step": 52
    },
    {
      "epoch": 0.02022659511031604,
      "grad_norm": 0.53515625,
      "learning_rate": 2.9797709923664122e-05,
      "loss": 0.0371,
      "step": 53
    },
    {
      "epoch": 0.020608228980322003,
      "grad_norm": 0.671875,
      "learning_rate": 2.9793893129770994e-05,
      "loss": 0.0539,
      "step": 54
    },
    {
      "epoch": 0.020989862850327966,
      "grad_norm": 0.494140625,
      "learning_rate": 2.9790076335877863e-05,
      "loss": 0.033,
      "step": 55
    },
    {
      "epoch": 0.021371496720333928,
      "grad_norm": 0.423828125,
      "learning_rate": 2.9786259541984735e-05,
      "loss": 0.0225,
      "step": 56
    },
    {
      "epoch": 0.021753130590339894,
      "grad_norm": 0.59375,
      "learning_rate": 2.9782442748091604e-05,
      "loss": 0.04,
      "step": 57
    },
    {
      "epoch": 0.022134764460345856,
      "grad_norm": 0.546875,
      "learning_rate": 2.9778625954198473e-05,
      "loss": 0.0322,
      "step": 58
    },
    {
      "epoch": 0.02251639833035182,
      "grad_norm": 0.515625,
      "learning_rate": 2.9774809160305345e-05,
      "loss": 0.0426,
      "step": 59
    },
    {
      "epoch": 0.02289803220035778,
      "grad_norm": 0.57421875,
      "learning_rate": 2.9770992366412214e-05,
      "loss": 0.0431,
      "step": 60
    },
    {
      "epoch": 0.023279666070363744,
      "grad_norm": 0.484375,
      "learning_rate": 2.9767175572519083e-05,
      "loss": 0.0324,
      "step": 61
    },
    {
      "epoch": 0.02366129994036971,
      "grad_norm": 0.7109375,
      "learning_rate": 2.9763358778625955e-05,
      "loss": 0.0385,
      "step": 62
    },
    {
      "epoch": 0.024042933810375672,
      "grad_norm": 0.46875,
      "learning_rate": 2.9759541984732824e-05,
      "loss": 0.0229,
      "step": 63
    },
    {
      "epoch": 0.024424567680381634,
      "grad_norm": 0.578125,
      "learning_rate": 2.9755725190839696e-05,
      "loss": 0.0216,
      "step": 64
    },
    {
      "epoch": 0.024806201550387597,
      "grad_norm": 0.494140625,
      "learning_rate": 2.9751908396946565e-05,
      "loss": 0.0354,
      "step": 65
    },
    {
      "epoch": 0.02518783542039356,
      "grad_norm": 0.49609375,
      "learning_rate": 2.9748091603053434e-05,
      "loss": 0.0327,
      "step": 66
    },
    {
      "epoch": 0.02556946929039952,
      "grad_norm": 0.62109375,
      "learning_rate": 2.9744274809160306e-05,
      "loss": 0.035,
      "step": 67
    },
    {
      "epoch": 0.025951103160405487,
      "grad_norm": 0.66015625,
      "learning_rate": 2.9740458015267175e-05,
      "loss": 0.0394,
      "step": 68
    },
    {
      "epoch": 0.02633273703041145,
      "grad_norm": 0.39453125,
      "learning_rate": 2.9736641221374048e-05,
      "loss": 0.0209,
      "step": 69
    },
    {
      "epoch": 0.026714370900417412,
      "grad_norm": 0.546875,
      "learning_rate": 2.9732824427480916e-05,
      "loss": 0.0264,
      "step": 70
    },
    {
      "epoch": 0.027096004770423374,
      "grad_norm": 0.498046875,
      "learning_rate": 2.9729007633587785e-05,
      "loss": 0.0295,
      "step": 71
    },
    {
      "epoch": 0.027477638640429337,
      "grad_norm": 0.51953125,
      "learning_rate": 2.9725190839694658e-05,
      "loss": 0.0316,
      "step": 72
    },
    {
      "epoch": 0.027859272510435303,
      "grad_norm": 0.40625,
      "learning_rate": 2.9721374045801526e-05,
      "loss": 0.024,
      "step": 73
    },
    {
      "epoch": 0.028240906380441265,
      "grad_norm": 0.53125,
      "learning_rate": 2.97175572519084e-05,
      "loss": 0.0278,
      "step": 74
    },
    {
      "epoch": 0.028622540250447227,
      "grad_norm": 0.470703125,
      "learning_rate": 2.9713740458015268e-05,
      "loss": 0.0273,
      "step": 75
    },
    {
      "epoch": 0.02900417412045319,
      "grad_norm": 0.482421875,
      "learning_rate": 2.9709923664122136e-05,
      "loss": 0.0225,
      "step": 76
    },
    {
      "epoch": 0.029385807990459152,
      "grad_norm": 0.56640625,
      "learning_rate": 2.970610687022901e-05,
      "loss": 0.0352,
      "step": 77
    },
    {
      "epoch": 0.029767441860465118,
      "grad_norm": 0.462890625,
      "learning_rate": 2.9702290076335878e-05,
      "loss": 0.026,
      "step": 78
    },
    {
      "epoch": 0.03014907573047108,
      "grad_norm": 0.5625,
      "learning_rate": 2.9698473282442746e-05,
      "loss": 0.0309,
      "step": 79
    },
    {
      "epoch": 0.030530709600477043,
      "grad_norm": 0.357421875,
      "learning_rate": 2.969465648854962e-05,
      "loss": 0.0209,
      "step": 80
    },
    {
      "epoch": 0.030912343470483005,
      "grad_norm": 0.361328125,
      "learning_rate": 2.9690839694656488e-05,
      "loss": 0.0315,
      "step": 81
    },
    {
      "epoch": 0.03129397734048897,
      "grad_norm": 0.42578125,
      "learning_rate": 2.968702290076336e-05,
      "loss": 0.0199,
      "step": 82
    },
    {
      "epoch": 0.03167561121049493,
      "grad_norm": 0.5859375,
      "learning_rate": 2.968320610687023e-05,
      "loss": 0.0228,
      "step": 83
    },
    {
      "epoch": 0.032057245080500896,
      "grad_norm": 0.33203125,
      "learning_rate": 2.9679389312977098e-05,
      "loss": 0.0196,
      "step": 84
    },
    {
      "epoch": 0.03243887895050686,
      "grad_norm": 0.34765625,
      "learning_rate": 2.967557251908397e-05,
      "loss": 0.0224,
      "step": 85
    },
    {
      "epoch": 0.03282051282051282,
      "grad_norm": 0.322265625,
      "learning_rate": 2.967175572519084e-05,
      "loss": 0.0239,
      "step": 86
    },
    {
      "epoch": 0.03320214669051878,
      "grad_norm": 0.330078125,
      "learning_rate": 2.966793893129771e-05,
      "loss": 0.0151,
      "step": 87
    },
    {
      "epoch": 0.033583780560524745,
      "grad_norm": 0.376953125,
      "learning_rate": 2.966412213740458e-05,
      "loss": 0.0232,
      "step": 88
    },
    {
      "epoch": 0.03396541443053071,
      "grad_norm": 0.328125,
      "learning_rate": 2.966030534351145e-05,
      "loss": 0.0201,
      "step": 89
    },
    {
      "epoch": 0.03434704830053667,
      "grad_norm": 0.388671875,
      "learning_rate": 2.965648854961832e-05,
      "loss": 0.03,
      "step": 90
    },
    {
      "epoch": 0.03472868217054263,
      "grad_norm": 0.388671875,
      "learning_rate": 2.965267175572519e-05,
      "loss": 0.015,
      "step": 91
    },
    {
      "epoch": 0.0351103160405486,
      "grad_norm": 0.56640625,
      "learning_rate": 2.964885496183206e-05,
      "loss": 0.0184,
      "step": 92
    },
    {
      "epoch": 0.035491949910554564,
      "grad_norm": 0.390625,
      "learning_rate": 2.964503816793893e-05,
      "loss": 0.0181,
      "step": 93
    },
    {
      "epoch": 0.035873583780560526,
      "grad_norm": 0.34765625,
      "learning_rate": 2.9641221374045803e-05,
      "loss": 0.0267,
      "step": 94
    },
    {
      "epoch": 0.03625521765056649,
      "grad_norm": 0.37890625,
      "learning_rate": 2.9637404580152675e-05,
      "loss": 0.015,
      "step": 95
    },
    {
      "epoch": 0.03663685152057245,
      "grad_norm": 0.55859375,
      "learning_rate": 2.9633587786259544e-05,
      "loss": 0.0358,
      "step": 96
    },
    {
      "epoch": 0.037018485390578414,
      "grad_norm": 0.486328125,
      "learning_rate": 2.9629770992366413e-05,
      "loss": 0.0174,
      "step": 97
    },
    {
      "epoch": 0.037400119260584376,
      "grad_norm": 0.326171875,
      "learning_rate": 2.9625954198473285e-05,
      "loss": 0.0114,
      "step": 98
    },
    {
      "epoch": 0.03778175313059034,
      "grad_norm": 0.6328125,
      "learning_rate": 2.9622137404580154e-05,
      "loss": 0.0279,
      "step": 99
    },
    {
      "epoch": 0.0381633870005963,
      "grad_norm": 0.353515625,
      "learning_rate": 2.9618320610687027e-05,
      "loss": 0.0191,
      "step": 100
    },
    {
      "epoch": 0.03854502087060226,
      "grad_norm": 0.310546875,
      "learning_rate": 2.9614503816793895e-05,
      "loss": 0.011,
      "step": 101
    },
    {
      "epoch": 0.038926654740608226,
      "grad_norm": 0.3203125,
      "learning_rate": 2.9610687022900764e-05,
      "loss": 0.0212,
      "step": 102
    },
    {
      "epoch": 0.039308288610614195,
      "grad_norm": 0.23046875,
      "learning_rate": 2.9606870229007637e-05,
      "loss": 0.009,
      "step": 103
    },
    {
      "epoch": 0.03968992248062016,
      "grad_norm": 0.44140625,
      "learning_rate": 2.9603053435114505e-05,
      "loss": 0.0137,
      "step": 104
    },
    {
      "epoch": 0.04007155635062612,
      "grad_norm": 0.431640625,
      "learning_rate": 2.9599236641221374e-05,
      "loss": 0.0307,
      "step": 105
    },
    {
      "epoch": 0.04045319022063208,
      "grad_norm": 0.70703125,
      "learning_rate": 2.9595419847328247e-05,
      "loss": 0.0134,
      "step": 106
    },
    {
      "epoch": 0.040834824090638044,
      "grad_norm": 0.546875,
      "learning_rate": 2.9591603053435115e-05,
      "loss": 0.0208,
      "step": 107
    },
    {
      "epoch": 0.04121645796064401,
      "grad_norm": 0.19921875,
      "learning_rate": 2.9587786259541988e-05,
      "loss": 0.0064,
      "step": 108
    },
    {
      "epoch": 0.04159809183064997,
      "grad_norm": 0.431640625,
      "learning_rate": 2.9583969465648857e-05,
      "loss": 0.0183,
      "step": 109
    },
    {
      "epoch": 0.04197972570065593,
      "grad_norm": 0.42578125,
      "learning_rate": 2.9580152671755725e-05,
      "loss": 0.0317,
      "step": 110
    },
    {
      "epoch": 0.042361359570661894,
      "grad_norm": 0.263671875,
      "learning_rate": 2.9576335877862598e-05,
      "loss": 0.0094,
      "step": 111
    },
    {
      "epoch": 0.042742993440667856,
      "grad_norm": 0.2333984375,
      "learning_rate": 2.9572519083969467e-05,
      "loss": 0.0129,
      "step": 112
    },
    {
      "epoch": 0.043124627310673826,
      "grad_norm": 0.53515625,
      "learning_rate": 2.956870229007634e-05,
      "loss": 0.0165,
      "step": 113
    },
    {
      "epoch": 0.04350626118067979,
      "grad_norm": 0.376953125,
      "learning_rate": 2.9564885496183208e-05,
      "loss": 0.0199,
      "step": 114
    },
    {
      "epoch": 0.04388789505068575,
      "grad_norm": 0.421875,
      "learning_rate": 2.9561068702290077e-05,
      "loss": 0.0185,
      "step": 115
    },
    {
      "epoch": 0.04426952892069171,
      "grad_norm": 0.53125,
      "learning_rate": 2.955725190839695e-05,
      "loss": 0.0203,
      "step": 116
    },
    {
      "epoch": 0.044651162790697675,
      "grad_norm": 0.81640625,
      "learning_rate": 2.9553435114503818e-05,
      "loss": 0.0144,
      "step": 117
    },
    {
      "epoch": 0.04503279666070364,
      "grad_norm": 0.271484375,
      "learning_rate": 2.9549618320610687e-05,
      "loss": 0.0243,
      "step": 118
    },
    {
      "epoch": 0.0454144305307096,
      "grad_norm": 0.515625,
      "learning_rate": 2.954580152671756e-05,
      "loss": 0.0129,
      "step": 119
    },
    {
      "epoch": 0.04579606440071556,
      "grad_norm": 0.3671875,
      "learning_rate": 2.9541984732824428e-05,
      "loss": 0.0125,
      "step": 120
    },
    {
      "epoch": 0.046177698270721525,
      "grad_norm": 0.291015625,
      "learning_rate": 2.95381679389313e-05,
      "loss": 0.0093,
      "step": 121
    },
    {
      "epoch": 0.04655933214072749,
      "grad_norm": 0.73046875,
      "learning_rate": 2.953435114503817e-05,
      "loss": 0.0098,
      "step": 122
    },
    {
      "epoch": 0.04694096601073345,
      "grad_norm": 0.455078125,
      "learning_rate": 2.9530534351145038e-05,
      "loss": 0.0267,
      "step": 123
    },
    {
      "epoch": 0.04732259988073942,
      "grad_norm": 0.431640625,
      "learning_rate": 2.952671755725191e-05,
      "loss": 0.0116,
      "step": 124
    },
    {
      "epoch": 0.04770423375074538,
      "grad_norm": 0.3671875,
      "learning_rate": 2.952290076335878e-05,
      "loss": 0.0211,
      "step": 125
    },
    {
      "epoch": 0.048085867620751344,
      "grad_norm": 0.322265625,
      "learning_rate": 2.951908396946565e-05,
      "loss": 0.0077,
      "step": 126
    },
    {
      "epoch": 0.048467501490757306,
      "grad_norm": 0.53515625,
      "learning_rate": 2.951526717557252e-05,
      "loss": 0.021,
      "step": 127
    },
    {
      "epoch": 0.04884913536076327,
      "grad_norm": 0.64453125,
      "learning_rate": 2.951145038167939e-05,
      "loss": 0.0149,
      "step": 128
    },
    {
      "epoch": 0.04923076923076923,
      "grad_norm": 0.53515625,
      "learning_rate": 2.950763358778626e-05,
      "loss": 0.0124,
      "step": 129
    },
    {
      "epoch": 0.04961240310077519,
      "grad_norm": 0.1552734375,
      "learning_rate": 2.950381679389313e-05,
      "loss": 0.0058,
      "step": 130
    },
    {
      "epoch": 0.049994036970781155,
      "grad_norm": 0.28515625,
      "learning_rate": 2.95e-05,
      "loss": 0.0113,
      "step": 131
    },
    {
      "epoch": 0.05037567084078712,
      "grad_norm": 0.404296875,
      "learning_rate": 2.949618320610687e-05,
      "loss": 0.0064,
      "step": 132
    },
    {
      "epoch": 0.05075730471079308,
      "grad_norm": 0.6015625,
      "learning_rate": 2.949236641221374e-05,
      "loss": 0.0104,
      "step": 133
    },
    {
      "epoch": 0.05113893858079904,
      "grad_norm": 0.265625,
      "learning_rate": 2.9488549618320612e-05,
      "loss": 0.0101,
      "step": 134
    },
    {
      "epoch": 0.05152057245080501,
      "grad_norm": 0.392578125,
      "learning_rate": 2.948473282442748e-05,
      "loss": 0.0082,
      "step": 135
    },
    {
      "epoch": 0.051902206320810974,
      "grad_norm": 0.7890625,
      "learning_rate": 2.948091603053435e-05,
      "loss": 0.0218,
      "step": 136
    },
    {
      "epoch": 0.05228384019081694,
      "grad_norm": 0.609375,
      "learning_rate": 2.9477099236641222e-05,
      "loss": 0.0261,
      "step": 137
    },
    {
      "epoch": 0.0526654740608229,
      "grad_norm": 0.341796875,
      "learning_rate": 2.947328244274809e-05,
      "loss": 0.0076,
      "step": 138
    },
    {
      "epoch": 0.05304710793082886,
      "grad_norm": 0.578125,
      "learning_rate": 2.9469465648854963e-05,
      "loss": 0.0198,
      "step": 139
    },
    {
      "epoch": 0.053428741800834824,
      "grad_norm": 0.380859375,
      "learning_rate": 2.9465648854961832e-05,
      "loss": 0.0167,
      "step": 140
    },
    {
      "epoch": 0.053810375670840786,
      "grad_norm": 0.5234375,
      "learning_rate": 2.94618320610687e-05,
      "loss": 0.0149,
      "step": 141
    },
    {
      "epoch": 0.05419200954084675,
      "grad_norm": 0.24609375,
      "learning_rate": 2.9458015267175573e-05,
      "loss": 0.0148,
      "step": 142
    },
    {
      "epoch": 0.05457364341085271,
      "grad_norm": 0.494140625,
      "learning_rate": 2.9454198473282442e-05,
      "loss": 0.017,
      "step": 143
    },
    {
      "epoch": 0.05495527728085867,
      "grad_norm": 0.224609375,
      "learning_rate": 2.945038167938931e-05,
      "loss": 0.0064,
      "step": 144
    },
    {
      "epoch": 0.055336911150864636,
      "grad_norm": 0.2451171875,
      "learning_rate": 2.9446564885496183e-05,
      "loss": 0.0064,
      "step": 145
    },
    {
      "epoch": 0.055718545020870605,
      "grad_norm": 0.3203125,
      "learning_rate": 2.9442748091603052e-05,
      "loss": 0.0096,
      "step": 146
    },
    {
      "epoch": 0.05610017889087657,
      "grad_norm": 0.482421875,
      "learning_rate": 2.9438931297709924e-05,
      "loss": 0.0146,
      "step": 147
    },
    {
      "epoch": 0.05648181276088253,
      "grad_norm": 0.431640625,
      "learning_rate": 2.9435114503816793e-05,
      "loss": 0.0135,
      "step": 148
    },
    {
      "epoch": 0.05686344663088849,
      "grad_norm": 0.357421875,
      "learning_rate": 2.9431297709923662e-05,
      "loss": 0.0154,
      "step": 149
    },
    {
      "epoch": 0.057245080500894455,
      "grad_norm": 0.3203125,
      "learning_rate": 2.9427480916030534e-05,
      "loss": 0.0072,
      "step": 150
    },
    {
      "epoch": 0.05762671437090042,
      "grad_norm": 0.2421875,
      "learning_rate": 2.9423664122137403e-05,
      "loss": 0.0102,
      "step": 151
    },
    {
      "epoch": 0.05800834824090638,
      "grad_norm": 0.279296875,
      "learning_rate": 2.9419847328244276e-05,
      "loss": 0.0084,
      "step": 152
    },
    {
      "epoch": 0.05838998211091234,
      "grad_norm": 0.220703125,
      "learning_rate": 2.9416030534351144e-05,
      "loss": 0.016,
      "step": 153
    },
    {
      "epoch": 0.058771615980918304,
      "grad_norm": 0.265625,
      "learning_rate": 2.9412213740458013e-05,
      "loss": 0.0147,
      "step": 154
    },
    {
      "epoch": 0.05915324985092427,
      "grad_norm": 0.193359375,
      "learning_rate": 2.9408396946564886e-05,
      "loss": 0.0169,
      "step": 155
    },
    {
      "epoch": 0.059534883720930236,
      "grad_norm": 0.283203125,
      "learning_rate": 2.9404580152671758e-05,
      "loss": 0.0106,
      "step": 156
    },
    {
      "epoch": 0.0599165175909362,
      "grad_norm": 0.1484375,
      "learning_rate": 2.9400763358778627e-05,
      "loss": 0.0168,
      "step": 157
    },
    {
      "epoch": 0.06029815146094216,
      "grad_norm": 0.169921875,
      "learning_rate": 2.93969465648855e-05,
      "loss": 0.0042,
      "step": 158
    },
    {
      "epoch": 0.06067978533094812,
      "grad_norm": 0.5234375,
      "learning_rate": 2.9393129770992368e-05,
      "loss": 0.0144,
      "step": 159
    },
    {
      "epoch": 0.061061419200954085,
      "grad_norm": 0.44921875,
      "learning_rate": 2.938931297709924e-05,
      "loss": 0.0148,
      "step": 160
    },
    {
      "epoch": 0.06144305307096005,
      "grad_norm": 0.09912109375,
      "learning_rate": 2.938549618320611e-05,
      "loss": 0.0111,
      "step": 161
    },
    {
      "epoch": 0.06182468694096601,
      "grad_norm": 0.1337890625,
      "learning_rate": 2.9381679389312978e-05,
      "loss": 0.0047,
      "step": 162
    },
    {
      "epoch": 0.06220632081097197,
      "grad_norm": 0.65234375,
      "learning_rate": 2.937786259541985e-05,
      "loss": 0.0131,
      "step": 163
    },
    {
      "epoch": 0.06258795468097794,
      "grad_norm": 0.2890625,
      "learning_rate": 2.937404580152672e-05,
      "loss": 0.0216,
      "step": 164
    },
    {
      "epoch": 0.0629695885509839,
      "grad_norm": 0.3984375,
      "learning_rate": 2.937022900763359e-05,
      "loss": 0.0132,
      "step": 165
    },
    {
      "epoch": 0.06335122242098987,
      "grad_norm": 0.10400390625,
      "learning_rate": 2.936641221374046e-05,
      "loss": 0.0044,
      "step": 166
    },
    {
      "epoch": 0.06373285629099583,
      "grad_norm": 0.1904296875,
      "learning_rate": 2.936259541984733e-05,
      "loss": 0.0041,
      "step": 167
    },
    {
      "epoch": 0.06411449016100179,
      "grad_norm": 0.0908203125,
      "learning_rate": 2.93587786259542e-05,
      "loss": 0.0093,
      "step": 168
    },
    {
      "epoch": 0.06449612403100775,
      "grad_norm": 0.453125,
      "learning_rate": 2.935496183206107e-05,
      "loss": 0.0101,
      "step": 169
    },
    {
      "epoch": 0.06487775790101372,
      "grad_norm": 0.33984375,
      "learning_rate": 2.935114503816794e-05,
      "loss": 0.0079,
      "step": 170
    },
    {
      "epoch": 0.06525939177101968,
      "grad_norm": 0.279296875,
      "learning_rate": 2.934732824427481e-05,
      "loss": 0.0145,
      "step": 171
    },
    {
      "epoch": 0.06564102564102564,
      "grad_norm": 0.1806640625,
      "learning_rate": 2.934351145038168e-05,
      "loss": 0.0041,
      "step": 172
    },
    {
      "epoch": 0.0660226595110316,
      "grad_norm": 0.75,
      "learning_rate": 2.9339694656488552e-05,
      "loss": 0.0242,
      "step": 173
    },
    {
      "epoch": 0.06640429338103757,
      "grad_norm": 0.15625,
      "learning_rate": 2.933587786259542e-05,
      "loss": 0.0027,
      "step": 174
    },
    {
      "epoch": 0.06678592725104353,
      "grad_norm": 0.17578125,
      "learning_rate": 2.933206106870229e-05,
      "loss": 0.0099,
      "step": 175
    },
    {
      "epoch": 0.06716756112104949,
      "grad_norm": 0.8125,
      "learning_rate": 2.9328244274809162e-05,
      "loss": 0.0156,
      "step": 176
    },
    {
      "epoch": 0.06754919499105545,
      "grad_norm": 0.41796875,
      "learning_rate": 2.932442748091603e-05,
      "loss": 0.0074,
      "step": 177
    },
    {
      "epoch": 0.06793082886106142,
      "grad_norm": 0.193359375,
      "learning_rate": 2.9320610687022903e-05,
      "loss": 0.0045,
      "step": 178
    },
    {
      "epoch": 0.06831246273106738,
      "grad_norm": 0.28515625,
      "learning_rate": 2.9316793893129772e-05,
      "loss": 0.0143,
      "step": 179
    },
    {
      "epoch": 0.06869409660107334,
      "grad_norm": 0.1494140625,
      "learning_rate": 2.931297709923664e-05,
      "loss": 0.0119,
      "step": 180
    },
    {
      "epoch": 0.0690757304710793,
      "grad_norm": 0.2109375,
      "learning_rate": 2.9309160305343513e-05,
      "loss": 0.0044,
      "step": 181
    },
    {
      "epoch": 0.06945736434108526,
      "grad_norm": 0.0576171875,
      "learning_rate": 2.9305343511450382e-05,
      "loss": 0.0017,
      "step": 182
    },
    {
      "epoch": 0.06983899821109124,
      "grad_norm": 0.11962890625,
      "learning_rate": 2.930152671755725e-05,
      "loss": 0.0041,
      "step": 183
    },
    {
      "epoch": 0.0702206320810972,
      "grad_norm": 0.1142578125,
      "learning_rate": 2.9297709923664123e-05,
      "loss": 0.0038,
      "step": 184
    },
    {
      "epoch": 0.07060226595110317,
      "grad_norm": 0.2470703125,
      "learning_rate": 2.9293893129770992e-05,
      "loss": 0.0056,
      "step": 185
    },
    {
      "epoch": 0.07098389982110913,
      "grad_norm": 0.28125,
      "learning_rate": 2.9290076335877864e-05,
      "loss": 0.0156,
      "step": 186
    },
    {
      "epoch": 0.07136553369111509,
      "grad_norm": 0.44921875,
      "learning_rate": 2.9286259541984733e-05,
      "loss": 0.0075,
      "step": 187
    },
    {
      "epoch": 0.07174716756112105,
      "grad_norm": 0.2490234375,
      "learning_rate": 2.9282442748091602e-05,
      "loss": 0.0042,
      "step": 188
    },
    {
      "epoch": 0.07212880143112702,
      "grad_norm": 0.248046875,
      "learning_rate": 2.9278625954198474e-05,
      "loss": 0.0025,
      "step": 189
    },
    {
      "epoch": 0.07251043530113298,
      "grad_norm": 0.08056640625,
      "learning_rate": 2.9274809160305343e-05,
      "loss": 0.0093,
      "step": 190
    },
    {
      "epoch": 0.07289206917113894,
      "grad_norm": 0.2265625,
      "learning_rate": 2.9270992366412216e-05,
      "loss": 0.0254,
      "step": 191
    },
    {
      "epoch": 0.0732737030411449,
      "grad_norm": 0.1923828125,
      "learning_rate": 2.9267175572519084e-05,
      "loss": 0.0049,
      "step": 192
    },
    {
      "epoch": 0.07365533691115086,
      "grad_norm": 0.83203125,
      "learning_rate": 2.9263358778625953e-05,
      "loss": 0.0235,
      "step": 193
    },
    {
      "epoch": 0.07403697078115683,
      "grad_norm": 0.205078125,
      "learning_rate": 2.9259541984732826e-05,
      "loss": 0.0129,
      "step": 194
    },
    {
      "epoch": 0.07441860465116279,
      "grad_norm": 0.37890625,
      "learning_rate": 2.9255725190839694e-05,
      "loss": 0.0058,
      "step": 195
    },
    {
      "epoch": 0.07480023852116875,
      "grad_norm": 0.87890625,
      "learning_rate": 2.9251908396946567e-05,
      "loss": 0.016,
      "step": 196
    },
    {
      "epoch": 0.07518187239117471,
      "grad_norm": 0.28125,
      "learning_rate": 2.9248091603053436e-05,
      "loss": 0.0057,
      "step": 197
    },
    {
      "epoch": 0.07556350626118068,
      "grad_norm": 1.2734375,
      "learning_rate": 2.9244274809160304e-05,
      "loss": 0.0125,
      "step": 198
    },
    {
      "epoch": 0.07594514013118664,
      "grad_norm": 0.11865234375,
      "learning_rate": 2.9240458015267177e-05,
      "loss": 0.0083,
      "step": 199
    },
    {
      "epoch": 0.0763267740011926,
      "grad_norm": 0.42578125,
      "learning_rate": 2.9236641221374046e-05,
      "loss": 0.0234,
      "step": 200
    },
    {
      "epoch": 0.07670840787119856,
      "grad_norm": 0.1943359375,
      "learning_rate": 2.9232824427480914e-05,
      "loss": 0.0024,
      "step": 201
    },
    {
      "epoch": 0.07709004174120453,
      "grad_norm": 0.251953125,
      "learning_rate": 2.9229007633587787e-05,
      "loss": 0.0114,
      "step": 202
    },
    {
      "epoch": 0.07747167561121049,
      "grad_norm": 0.1484375,
      "learning_rate": 2.9225190839694656e-05,
      "loss": 0.0029,
      "step": 203
    },
    {
      "epoch": 0.07785330948121645,
      "grad_norm": 0.197265625,
      "learning_rate": 2.9221374045801528e-05,
      "loss": 0.0067,
      "step": 204
    },
    {
      "epoch": 0.07823494335122243,
      "grad_norm": 0.6328125,
      "learning_rate": 2.9217557251908397e-05,
      "loss": 0.0127,
      "step": 205
    },
    {
      "epoch": 0.07861657722122839,
      "grad_norm": 0.7734375,
      "learning_rate": 2.9213740458015266e-05,
      "loss": 0.0151,
      "step": 206
    },
    {
      "epoch": 0.07899821109123435,
      "grad_norm": 0.29296875,
      "learning_rate": 2.9209923664122138e-05,
      "loss": 0.0162,
      "step": 207
    },
    {
      "epoch": 0.07937984496124031,
      "grad_norm": 0.326171875,
      "learning_rate": 2.9206106870229007e-05,
      "loss": 0.0147,
      "step": 208
    },
    {
      "epoch": 0.07976147883124628,
      "grad_norm": 0.435546875,
      "learning_rate": 2.920229007633588e-05,
      "loss": 0.0049,
      "step": 209
    },
    {
      "epoch": 0.08014311270125224,
      "grad_norm": 0.470703125,
      "learning_rate": 2.9198473282442748e-05,
      "loss": 0.0073,
      "step": 210
    },
    {
      "epoch": 0.0805247465712582,
      "grad_norm": 0.1689453125,
      "learning_rate": 2.9194656488549617e-05,
      "loss": 0.0099,
      "step": 211
    },
    {
      "epoch": 0.08090638044126416,
      "grad_norm": 0.50390625,
      "learning_rate": 2.919083969465649e-05,
      "loss": 0.0067,
      "step": 212
    },
    {
      "epoch": 0.08128801431127013,
      "grad_norm": 0.322265625,
      "learning_rate": 2.9187022900763358e-05,
      "loss": 0.0081,
      "step": 213
    },
    {
      "epoch": 0.08166964818127609,
      "grad_norm": 0.1962890625,
      "learning_rate": 2.9183206106870227e-05,
      "loss": 0.006,
      "step": 214
    },
    {
      "epoch": 0.08205128205128205,
      "grad_norm": 0.314453125,
      "learning_rate": 2.91793893129771e-05,
      "loss": 0.0046,
      "step": 215
    },
    {
      "epoch": 0.08243291592128801,
      "grad_norm": 0.109375,
      "learning_rate": 2.9175572519083968e-05,
      "loss": 0.0164,
      "step": 216
    },
    {
      "epoch": 0.08281454979129398,
      "grad_norm": 0.283203125,
      "learning_rate": 2.917175572519084e-05,
      "loss": 0.0088,
      "step": 217
    },
    {
      "epoch": 0.08319618366129994,
      "grad_norm": 0.66015625,
      "learning_rate": 2.9167938931297712e-05,
      "loss": 0.0071,
      "step": 218
    },
    {
      "epoch": 0.0835778175313059,
      "grad_norm": 0.3125,
      "learning_rate": 2.916412213740458e-05,
      "loss": 0.0037,
      "step": 219
    },
    {
      "epoch": 0.08395945140131186,
      "grad_norm": 0.1181640625,
      "learning_rate": 2.9160305343511453e-05,
      "loss": 0.0025,
      "step": 220
    },
    {
      "epoch": 0.08434108527131783,
      "grad_norm": 0.30859375,
      "learning_rate": 2.9156488549618322e-05,
      "loss": 0.017,
      "step": 221
    },
    {
      "epoch": 0.08472271914132379,
      "grad_norm": 0.384765625,
      "learning_rate": 2.9152671755725195e-05,
      "loss": 0.0139,
      "step": 222
    },
    {
      "epoch": 0.08510435301132975,
      "grad_norm": 0.09765625,
      "learning_rate": 2.9148854961832063e-05,
      "loss": 0.0021,
      "step": 223
    },
    {
      "epoch": 0.08548598688133571,
      "grad_norm": 0.419921875,
      "learning_rate": 2.9145038167938932e-05,
      "loss": 0.0046,
      "step": 224
    },
    {
      "epoch": 0.08586762075134168,
      "grad_norm": 0.4375,
      "learning_rate": 2.9141221374045805e-05,
      "loss": 0.0042,
      "step": 225
    },
    {
      "epoch": 0.08624925462134765,
      "grad_norm": 0.271484375,
      "learning_rate": 2.9137404580152673e-05,
      "loss": 0.0135,
      "step": 226
    },
    {
      "epoch": 0.08663088849135361,
      "grad_norm": 0.201171875,
      "learning_rate": 2.9133587786259542e-05,
      "loss": 0.0086,
      "step": 227
    },
    {
      "epoch": 0.08701252236135958,
      "grad_norm": 0.2294921875,
      "learning_rate": 2.9129770992366415e-05,
      "loss": 0.0201,
      "step": 228
    },
    {
      "epoch": 0.08739415623136554,
      "grad_norm": 0.337890625,
      "learning_rate": 2.9125954198473283e-05,
      "loss": 0.0078,
      "step": 229
    },
    {
      "epoch": 0.0877757901013715,
      "grad_norm": 0.263671875,
      "learning_rate": 2.9122137404580156e-05,
      "loss": 0.0213,
      "step": 230
    },
    {
      "epoch": 0.08815742397137746,
      "grad_norm": 0.283203125,
      "learning_rate": 2.9118320610687025e-05,
      "loss": 0.0036,
      "step": 231
    },
    {
      "epoch": 0.08853905784138343,
      "grad_norm": 0.1513671875,
      "learning_rate": 2.9114503816793893e-05,
      "loss": 0.006,
      "step": 232
    },
    {
      "epoch": 0.08892069171138939,
      "grad_norm": 0.1572265625,
      "learning_rate": 2.9110687022900766e-05,
      "loss": 0.0022,
      "step": 233
    },
    {
      "epoch": 0.08930232558139535,
      "grad_norm": 0.07861328125,
      "learning_rate": 2.9106870229007635e-05,
      "loss": 0.0021,
      "step": 234
    },
    {
      "epoch": 0.08968395945140131,
      "grad_norm": 0.130859375,
      "learning_rate": 2.9103053435114507e-05,
      "loss": 0.0096,
      "step": 235
    },
    {
      "epoch": 0.09006559332140728,
      "grad_norm": 0.310546875,
      "learning_rate": 2.9099236641221376e-05,
      "loss": 0.0108,
      "step": 236
    },
    {
      "epoch": 0.09044722719141324,
      "grad_norm": 0.39453125,
      "learning_rate": 2.9095419847328245e-05,
      "loss": 0.0024,
      "step": 237
    },
    {
      "epoch": 0.0908288610614192,
      "grad_norm": 0.0732421875,
      "learning_rate": 2.9091603053435117e-05,
      "loss": 0.0023,
      "step": 238
    },
    {
      "epoch": 0.09121049493142516,
      "grad_norm": 0.0732421875,
      "learning_rate": 2.9087786259541986e-05,
      "loss": 0.0093,
      "step": 239
    },
    {
      "epoch": 0.09159212880143112,
      "grad_norm": 0.0634765625,
      "learning_rate": 2.9083969465648855e-05,
      "loss": 0.0015,
      "step": 240
    }
  ],
  "logging_steps": 1,
  "max_steps": 7860,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
