{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.07420289855072464,
  "eval_steps": 500,
  "global_step": 130,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005707915273132665,
      "grad_norm": 4.3125,
      "learning_rate": 2.9994288977727013e-05,
      "loss": 0.5774,
      "step": 1
    },
    {
      "epoch": 0.001141583054626533,
      "grad_norm": 1.03125,
      "learning_rate": 2.998857795545403e-05,
      "loss": 0.6062,
      "step": 2
    },
    {
      "epoch": 0.0017123745819397993,
      "grad_norm": 1.15625,
      "learning_rate": 2.998286693318104e-05,
      "loss": 0.5913,
      "step": 3
    },
    {
      "epoch": 0.002283166109253066,
      "grad_norm": 1.25,
      "learning_rate": 2.9977155910908053e-05,
      "loss": 0.51,
      "step": 4
    },
    {
      "epoch": 0.002853957636566332,
      "grad_norm": 1.2890625,
      "learning_rate": 2.9971444888635065e-05,
      "loss": 0.4734,
      "step": 5
    },
    {
      "epoch": 0.0034247491638795986,
      "grad_norm": 1.3125,
      "learning_rate": 2.996573386636208e-05,
      "loss": 0.4566,
      "step": 6
    },
    {
      "epoch": 0.003995540691192865,
      "grad_norm": 1.265625,
      "learning_rate": 2.9960022844089092e-05,
      "loss": 0.3998,
      "step": 7
    },
    {
      "epoch": 0.004566332218506132,
      "grad_norm": 1.34375,
      "learning_rate": 2.9954311821816108e-05,
      "loss": 0.3361,
      "step": 8
    },
    {
      "epoch": 0.005137123745819398,
      "grad_norm": 1.2890625,
      "learning_rate": 2.994860079954312e-05,
      "loss": 0.2833,
      "step": 9
    },
    {
      "epoch": 0.005707915273132664,
      "grad_norm": 1.390625,
      "learning_rate": 2.9942889777270132e-05,
      "loss": 0.2787,
      "step": 10
    },
    {
      "epoch": 0.006278706800445931,
      "grad_norm": 1.2578125,
      "learning_rate": 2.9937178754997144e-05,
      "loss": 0.2515,
      "step": 11
    },
    {
      "epoch": 0.006849498327759197,
      "grad_norm": 1.5,
      "learning_rate": 2.9931467732724156e-05,
      "loss": 0.2385,
      "step": 12
    },
    {
      "epoch": 0.007420289855072463,
      "grad_norm": 2.0,
      "learning_rate": 2.9925756710451172e-05,
      "loss": 0.2067,
      "step": 13
    },
    {
      "epoch": 0.00799108138238573,
      "grad_norm": 2.1875,
      "learning_rate": 2.9920045688178184e-05,
      "loss": 0.2204,
      "step": 14
    },
    {
      "epoch": 0.008561872909698997,
      "grad_norm": 1.203125,
      "learning_rate": 2.99143346659052e-05,
      "loss": 0.1557,
      "step": 15
    },
    {
      "epoch": 0.009132664437012264,
      "grad_norm": 1.1953125,
      "learning_rate": 2.990862364363221e-05,
      "loss": 0.1763,
      "step": 16
    },
    {
      "epoch": 0.009703455964325529,
      "grad_norm": 0.9921875,
      "learning_rate": 2.9902912621359224e-05,
      "loss": 0.1211,
      "step": 17
    },
    {
      "epoch": 0.010274247491638796,
      "grad_norm": 0.8828125,
      "learning_rate": 2.9897201599086236e-05,
      "loss": 0.1077,
      "step": 18
    },
    {
      "epoch": 0.010845039018952063,
      "grad_norm": 0.83984375,
      "learning_rate": 2.989149057681325e-05,
      "loss": 0.1172,
      "step": 19
    },
    {
      "epoch": 0.011415830546265328,
      "grad_norm": 0.75,
      "learning_rate": 2.9885779554540263e-05,
      "loss": 0.1087,
      "step": 20
    },
    {
      "epoch": 0.011986622073578595,
      "grad_norm": 0.75390625,
      "learning_rate": 2.9880068532267276e-05,
      "loss": 0.0914,
      "step": 21
    },
    {
      "epoch": 0.012557413600891862,
      "grad_norm": 0.578125,
      "learning_rate": 2.987435750999429e-05,
      "loss": 0.0737,
      "step": 22
    },
    {
      "epoch": 0.013128205128205127,
      "grad_norm": 0.91796875,
      "learning_rate": 2.9868646487721303e-05,
      "loss": 0.0747,
      "step": 23
    },
    {
      "epoch": 0.013698996655518395,
      "grad_norm": 0.71875,
      "learning_rate": 2.9862935465448315e-05,
      "loss": 0.0776,
      "step": 24
    },
    {
      "epoch": 0.014269788182831662,
      "grad_norm": 0.609375,
      "learning_rate": 2.9857224443175327e-05,
      "loss": 0.058,
      "step": 25
    },
    {
      "epoch": 0.014840579710144927,
      "grad_norm": 0.58984375,
      "learning_rate": 2.9851513420902343e-05,
      "loss": 0.0568,
      "step": 26
    },
    {
      "epoch": 0.015411371237458194,
      "grad_norm": 0.7109375,
      "learning_rate": 2.9845802398629355e-05,
      "loss": 0.0753,
      "step": 27
    },
    {
      "epoch": 0.01598216276477146,
      "grad_norm": 0.56640625,
      "learning_rate": 2.984009137635637e-05,
      "loss": 0.0467,
      "step": 28
    },
    {
      "epoch": 0.016552954292084726,
      "grad_norm": 0.49609375,
      "learning_rate": 2.9834380354083383e-05,
      "loss": 0.0459,
      "step": 29
    },
    {
      "epoch": 0.017123745819397993,
      "grad_norm": 0.45703125,
      "learning_rate": 2.9828669331810398e-05,
      "loss": 0.0587,
      "step": 30
    },
    {
      "epoch": 0.01769453734671126,
      "grad_norm": 0.41796875,
      "learning_rate": 2.9822958309537407e-05,
      "loss": 0.0408,
      "step": 31
    },
    {
      "epoch": 0.018265328874024527,
      "grad_norm": 0.4921875,
      "learning_rate": 2.981724728726442e-05,
      "loss": 0.0463,
      "step": 32
    },
    {
      "epoch": 0.018836120401337794,
      "grad_norm": 0.5625,
      "learning_rate": 2.9811536264991434e-05,
      "loss": 0.0345,
      "step": 33
    },
    {
      "epoch": 0.019406911928651058,
      "grad_norm": 0.482421875,
      "learning_rate": 2.9805825242718447e-05,
      "loss": 0.0616,
      "step": 34
    },
    {
      "epoch": 0.019977703455964325,
      "grad_norm": 0.33203125,
      "learning_rate": 2.9800114220445462e-05,
      "loss": 0.0383,
      "step": 35
    },
    {
      "epoch": 0.020548494983277592,
      "grad_norm": 0.34375,
      "learning_rate": 2.9794403198172474e-05,
      "loss": 0.0347,
      "step": 36
    },
    {
      "epoch": 0.02111928651059086,
      "grad_norm": 0.416015625,
      "learning_rate": 2.9788692175899486e-05,
      "loss": 0.0381,
      "step": 37
    },
    {
      "epoch": 0.021690078037904126,
      "grad_norm": 0.30078125,
      "learning_rate": 2.97829811536265e-05,
      "loss": 0.0377,
      "step": 38
    },
    {
      "epoch": 0.022260869565217393,
      "grad_norm": 0.439453125,
      "learning_rate": 2.9777270131353514e-05,
      "loss": 0.0423,
      "step": 39
    },
    {
      "epoch": 0.022831661092530656,
      "grad_norm": 0.328125,
      "learning_rate": 2.9771559109080526e-05,
      "loss": 0.0498,
      "step": 40
    },
    {
      "epoch": 0.023402452619843923,
      "grad_norm": 0.322265625,
      "learning_rate": 2.9765848086807538e-05,
      "loss": 0.0327,
      "step": 41
    },
    {
      "epoch": 0.02397324414715719,
      "grad_norm": 0.35546875,
      "learning_rate": 2.9760137064534554e-05,
      "loss": 0.0562,
      "step": 42
    },
    {
      "epoch": 0.024544035674470457,
      "grad_norm": 0.37109375,
      "learning_rate": 2.9754426042261566e-05,
      "loss": 0.0381,
      "step": 43
    },
    {
      "epoch": 0.025114827201783724,
      "grad_norm": 0.70703125,
      "learning_rate": 2.9748715019988578e-05,
      "loss": 0.0566,
      "step": 44
    },
    {
      "epoch": 0.02568561872909699,
      "grad_norm": 0.515625,
      "learning_rate": 2.974300399771559e-05,
      "loss": 0.0481,
      "step": 45
    },
    {
      "epoch": 0.026256410256410255,
      "grad_norm": 0.412109375,
      "learning_rate": 2.9737292975442606e-05,
      "loss": 0.0498,
      "step": 46
    },
    {
      "epoch": 0.026827201783723522,
      "grad_norm": 0.5234375,
      "learning_rate": 2.9731581953169618e-05,
      "loss": 0.0403,
      "step": 47
    },
    {
      "epoch": 0.02739799331103679,
      "grad_norm": 0.349609375,
      "learning_rate": 2.9725870930896633e-05,
      "loss": 0.0504,
      "step": 48
    },
    {
      "epoch": 0.027968784838350056,
      "grad_norm": 1.078125,
      "learning_rate": 2.9720159908623645e-05,
      "loss": 0.0453,
      "step": 49
    },
    {
      "epoch": 0.028539576365663323,
      "grad_norm": 0.3671875,
      "learning_rate": 2.971444888635066e-05,
      "loss": 0.0454,
      "step": 50
    },
    {
      "epoch": 0.02911036789297659,
      "grad_norm": 0.345703125,
      "learning_rate": 2.970873786407767e-05,
      "loss": 0.0393,
      "step": 51
    },
    {
      "epoch": 0.029681159420289854,
      "grad_norm": 0.3046875,
      "learning_rate": 2.970302684180468e-05,
      "loss": 0.0349,
      "step": 52
    },
    {
      "epoch": 0.03025195094760312,
      "grad_norm": 0.400390625,
      "learning_rate": 2.9697315819531697e-05,
      "loss": 0.0356,
      "step": 53
    },
    {
      "epoch": 0.030822742474916388,
      "grad_norm": 0.32421875,
      "learning_rate": 2.969160479725871e-05,
      "loss": 0.0341,
      "step": 54
    },
    {
      "epoch": 0.031393534002229655,
      "grad_norm": 0.216796875,
      "learning_rate": 2.9685893774985725e-05,
      "loss": 0.0222,
      "step": 55
    },
    {
      "epoch": 0.03196432552954292,
      "grad_norm": 0.298828125,
      "learning_rate": 2.9680182752712737e-05,
      "loss": 0.0353,
      "step": 56
    },
    {
      "epoch": 0.03253511705685619,
      "grad_norm": 0.24609375,
      "learning_rate": 2.967447173043975e-05,
      "loss": 0.0277,
      "step": 57
    },
    {
      "epoch": 0.03310590858416945,
      "grad_norm": 0.353515625,
      "learning_rate": 2.966876070816676e-05,
      "loss": 0.0358,
      "step": 58
    },
    {
      "epoch": 0.03367670011148272,
      "grad_norm": 0.33203125,
      "learning_rate": 2.9663049685893777e-05,
      "loss": 0.0316,
      "step": 59
    },
    {
      "epoch": 0.034247491638795986,
      "grad_norm": 0.345703125,
      "learning_rate": 2.965733866362079e-05,
      "loss": 0.0285,
      "step": 60
    },
    {
      "epoch": 0.03481828316610925,
      "grad_norm": 0.439453125,
      "learning_rate": 2.96516276413478e-05,
      "loss": 0.0301,
      "step": 61
    },
    {
      "epoch": 0.03538907469342252,
      "grad_norm": 0.26171875,
      "learning_rate": 2.9645916619074816e-05,
      "loss": 0.0282,
      "step": 62
    },
    {
      "epoch": 0.035959866220735784,
      "grad_norm": 0.330078125,
      "learning_rate": 2.964020559680183e-05,
      "loss": 0.0413,
      "step": 63
    },
    {
      "epoch": 0.036530657748049054,
      "grad_norm": 0.5390625,
      "learning_rate": 2.963449457452884e-05,
      "loss": 0.0398,
      "step": 64
    },
    {
      "epoch": 0.03710144927536232,
      "grad_norm": 0.30859375,
      "learning_rate": 2.9628783552255853e-05,
      "loss": 0.0343,
      "step": 65
    },
    {
      "epoch": 0.03767224080267559,
      "grad_norm": 0.376953125,
      "learning_rate": 2.9623072529982868e-05,
      "loss": 0.0354,
      "step": 66
    },
    {
      "epoch": 0.03824303232998885,
      "grad_norm": 0.341796875,
      "learning_rate": 2.961736150770988e-05,
      "loss": 0.0536,
      "step": 67
    },
    {
      "epoch": 0.038813823857302115,
      "grad_norm": 0.35546875,
      "learning_rate": 2.9611650485436896e-05,
      "loss": 0.0234,
      "step": 68
    },
    {
      "epoch": 0.039384615384615386,
      "grad_norm": 0.259765625,
      "learning_rate": 2.9605939463163908e-05,
      "loss": 0.0421,
      "step": 69
    },
    {
      "epoch": 0.03995540691192865,
      "grad_norm": 0.30859375,
      "learning_rate": 2.9600228440890923e-05,
      "loss": 0.0265,
      "step": 70
    },
    {
      "epoch": 0.04052619843924192,
      "grad_norm": 0.251953125,
      "learning_rate": 2.9594517418617932e-05,
      "loss": 0.0274,
      "step": 71
    },
    {
      "epoch": 0.041096989966555184,
      "grad_norm": 0.298828125,
      "learning_rate": 2.9588806396344944e-05,
      "loss": 0.0349,
      "step": 72
    },
    {
      "epoch": 0.04166778149386845,
      "grad_norm": 0.53125,
      "learning_rate": 2.958309537407196e-05,
      "loss": 0.0383,
      "step": 73
    },
    {
      "epoch": 0.04223857302118172,
      "grad_norm": 0.310546875,
      "learning_rate": 2.9577384351798972e-05,
      "loss": 0.0303,
      "step": 74
    },
    {
      "epoch": 0.04280936454849498,
      "grad_norm": 0.302734375,
      "learning_rate": 2.9571673329525987e-05,
      "loss": 0.0244,
      "step": 75
    },
    {
      "epoch": 0.04338015607580825,
      "grad_norm": 0.76171875,
      "learning_rate": 2.9565962307253e-05,
      "loss": 0.0216,
      "step": 76
    },
    {
      "epoch": 0.043950947603121515,
      "grad_norm": 0.36328125,
      "learning_rate": 2.956025128498001e-05,
      "loss": 0.0323,
      "step": 77
    },
    {
      "epoch": 0.044521739130434786,
      "grad_norm": 0.546875,
      "learning_rate": 2.9554540262707024e-05,
      "loss": 0.0385,
      "step": 78
    },
    {
      "epoch": 0.04509253065774805,
      "grad_norm": 0.380859375,
      "learning_rate": 2.954882924043404e-05,
      "loss": 0.0337,
      "step": 79
    },
    {
      "epoch": 0.04566332218506131,
      "grad_norm": 0.318359375,
      "learning_rate": 2.954311821816105e-05,
      "loss": 0.0239,
      "step": 80
    },
    {
      "epoch": 0.04623411371237458,
      "grad_norm": 0.48046875,
      "learning_rate": 2.9537407195888063e-05,
      "loss": 0.0461,
      "step": 81
    },
    {
      "epoch": 0.04680490523968785,
      "grad_norm": 0.38671875,
      "learning_rate": 2.953169617361508e-05,
      "loss": 0.0375,
      "step": 82
    },
    {
      "epoch": 0.04737569676700112,
      "grad_norm": 0.84375,
      "learning_rate": 2.952598515134209e-05,
      "loss": 0.0303,
      "step": 83
    },
    {
      "epoch": 0.04794648829431438,
      "grad_norm": 0.2392578125,
      "learning_rate": 2.9520274129069103e-05,
      "loss": 0.0323,
      "step": 84
    },
    {
      "epoch": 0.048517279821627644,
      "grad_norm": 0.216796875,
      "learning_rate": 2.9514563106796115e-05,
      "loss": 0.0204,
      "step": 85
    },
    {
      "epoch": 0.049088071348940915,
      "grad_norm": 0.291015625,
      "learning_rate": 2.950885208452313e-05,
      "loss": 0.023,
      "step": 86
    },
    {
      "epoch": 0.04965886287625418,
      "grad_norm": 0.53125,
      "learning_rate": 2.9503141062250143e-05,
      "loss": 0.0334,
      "step": 87
    },
    {
      "epoch": 0.05022965440356745,
      "grad_norm": 0.353515625,
      "learning_rate": 2.949743003997716e-05,
      "loss": 0.0263,
      "step": 88
    },
    {
      "epoch": 0.05080044593088071,
      "grad_norm": 0.369140625,
      "learning_rate": 2.949171901770417e-05,
      "loss": 0.0398,
      "step": 89
    },
    {
      "epoch": 0.05137123745819398,
      "grad_norm": 0.236328125,
      "learning_rate": 2.9486007995431186e-05,
      "loss": 0.0184,
      "step": 90
    },
    {
      "epoch": 0.051942028985507246,
      "grad_norm": 0.33203125,
      "learning_rate": 2.9480296973158195e-05,
      "loss": 0.022,
      "step": 91
    },
    {
      "epoch": 0.05251282051282051,
      "grad_norm": 0.349609375,
      "learning_rate": 2.9474585950885207e-05,
      "loss": 0.0342,
      "step": 92
    },
    {
      "epoch": 0.05308361204013378,
      "grad_norm": 0.34375,
      "learning_rate": 2.9468874928612222e-05,
      "loss": 0.0266,
      "step": 93
    },
    {
      "epoch": 0.053654403567447044,
      "grad_norm": 0.2314453125,
      "learning_rate": 2.9463163906339235e-05,
      "loss": 0.0169,
      "step": 94
    },
    {
      "epoch": 0.054225195094760315,
      "grad_norm": 0.2197265625,
      "learning_rate": 2.945745288406625e-05,
      "loss": 0.0192,
      "step": 95
    },
    {
      "epoch": 0.05479598662207358,
      "grad_norm": 0.431640625,
      "learning_rate": 2.9451741861793262e-05,
      "loss": 0.0251,
      "step": 96
    },
    {
      "epoch": 0.05536677814938684,
      "grad_norm": 0.35546875,
      "learning_rate": 2.9446030839520274e-05,
      "loss": 0.0256,
      "step": 97
    },
    {
      "epoch": 0.05593756967670011,
      "grad_norm": 0.515625,
      "learning_rate": 2.9440319817247286e-05,
      "loss": 0.0265,
      "step": 98
    },
    {
      "epoch": 0.056508361204013376,
      "grad_norm": 0.322265625,
      "learning_rate": 2.9434608794974302e-05,
      "loss": 0.0217,
      "step": 99
    },
    {
      "epoch": 0.057079152731326646,
      "grad_norm": 0.302734375,
      "learning_rate": 2.9428897772701314e-05,
      "loss": 0.0217,
      "step": 100
    },
    {
      "epoch": 0.05764994425863991,
      "grad_norm": 0.279296875,
      "learning_rate": 2.9423186750428326e-05,
      "loss": 0.0153,
      "step": 101
    },
    {
      "epoch": 0.05822073578595318,
      "grad_norm": 0.3515625,
      "learning_rate": 2.941747572815534e-05,
      "loss": 0.025,
      "step": 102
    },
    {
      "epoch": 0.058791527313266444,
      "grad_norm": 0.32421875,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 0.0238,
      "step": 103
    },
    {
      "epoch": 0.05936231884057971,
      "grad_norm": 0.197265625,
      "learning_rate": 2.9406053683609366e-05,
      "loss": 0.0156,
      "step": 104
    },
    {
      "epoch": 0.05993311036789298,
      "grad_norm": 0.330078125,
      "learning_rate": 2.9400342661336378e-05,
      "loss": 0.0214,
      "step": 105
    },
    {
      "epoch": 0.06050390189520624,
      "grad_norm": 0.208984375,
      "learning_rate": 2.9394631639063393e-05,
      "loss": 0.0218,
      "step": 106
    },
    {
      "epoch": 0.06107469342251951,
      "grad_norm": 0.2080078125,
      "learning_rate": 2.9388920616790406e-05,
      "loss": 0.0151,
      "step": 107
    },
    {
      "epoch": 0.061645484949832775,
      "grad_norm": 0.298828125,
      "learning_rate": 2.938320959451742e-05,
      "loss": 0.0281,
      "step": 108
    },
    {
      "epoch": 0.062216276477146046,
      "grad_norm": 0.302734375,
      "learning_rate": 2.9377498572244433e-05,
      "loss": 0.0177,
      "step": 109
    },
    {
      "epoch": 0.06278706800445931,
      "grad_norm": 0.28515625,
      "learning_rate": 2.937178754997145e-05,
      "loss": 0.0202,
      "step": 110
    },
    {
      "epoch": 0.06335785953177257,
      "grad_norm": 0.224609375,
      "learning_rate": 2.9366076527698457e-05,
      "loss": 0.0171,
      "step": 111
    },
    {
      "epoch": 0.06392865105908584,
      "grad_norm": 0.251953125,
      "learning_rate": 2.936036550542547e-05,
      "loss": 0.0236,
      "step": 112
    },
    {
      "epoch": 0.06449944258639911,
      "grad_norm": 0.26953125,
      "learning_rate": 2.9354654483152485e-05,
      "loss": 0.0255,
      "step": 113
    },
    {
      "epoch": 0.06507023411371238,
      "grad_norm": 0.27734375,
      "learning_rate": 2.9348943460879497e-05,
      "loss": 0.0232,
      "step": 114
    },
    {
      "epoch": 0.06564102564102564,
      "grad_norm": 0.22265625,
      "learning_rate": 2.9343232438606513e-05,
      "loss": 0.0163,
      "step": 115
    },
    {
      "epoch": 0.0662118171683389,
      "grad_norm": 0.1689453125,
      "learning_rate": 2.9337521416333525e-05,
      "loss": 0.0106,
      "step": 116
    },
    {
      "epoch": 0.06678260869565217,
      "grad_norm": 0.369140625,
      "learning_rate": 2.933181039406054e-05,
      "loss": 0.0167,
      "step": 117
    },
    {
      "epoch": 0.06735340022296545,
      "grad_norm": 0.244140625,
      "learning_rate": 2.932609937178755e-05,
      "loss": 0.0156,
      "step": 118
    },
    {
      "epoch": 0.06792419175027871,
      "grad_norm": 0.1748046875,
      "learning_rate": 2.9320388349514565e-05,
      "loss": 0.009,
      "step": 119
    },
    {
      "epoch": 0.06849498327759197,
      "grad_norm": 0.248046875,
      "learning_rate": 2.9314677327241577e-05,
      "loss": 0.0211,
      "step": 120
    },
    {
      "epoch": 0.06906577480490524,
      "grad_norm": 0.23828125,
      "learning_rate": 2.930896630496859e-05,
      "loss": 0.0119,
      "step": 121
    },
    {
      "epoch": 0.0696365663322185,
      "grad_norm": 0.26953125,
      "learning_rate": 2.9303255282695604e-05,
      "loss": 0.0151,
      "step": 122
    },
    {
      "epoch": 0.07020735785953178,
      "grad_norm": 0.1689453125,
      "learning_rate": 2.9297544260422616e-05,
      "loss": 0.0077,
      "step": 123
    },
    {
      "epoch": 0.07077814938684504,
      "grad_norm": 0.3203125,
      "learning_rate": 2.929183323814963e-05,
      "loss": 0.024,
      "step": 124
    },
    {
      "epoch": 0.0713489409141583,
      "grad_norm": 0.2412109375,
      "learning_rate": 2.928612221587664e-05,
      "loss": 0.0122,
      "step": 125
    },
    {
      "epoch": 0.07191973244147157,
      "grad_norm": 0.263671875,
      "learning_rate": 2.9280411193603656e-05,
      "loss": 0.0186,
      "step": 126
    },
    {
      "epoch": 0.07249052396878485,
      "grad_norm": 0.150390625,
      "learning_rate": 2.9274700171330668e-05,
      "loss": 0.0086,
      "step": 127
    },
    {
      "epoch": 0.07306131549609811,
      "grad_norm": 0.2197265625,
      "learning_rate": 2.9268989149057684e-05,
      "loss": 0.0168,
      "step": 128
    },
    {
      "epoch": 0.07363210702341137,
      "grad_norm": 0.224609375,
      "learning_rate": 2.9263278126784696e-05,
      "loss": 0.0121,
      "step": 129
    },
    {
      "epoch": 0.07420289855072464,
      "grad_norm": 0.314453125,
      "learning_rate": 2.925756710451171e-05,
      "loss": 0.0243,
      "step": 130
    }
  ],
  "logging_steps": 1,
  "max_steps": 5253,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
