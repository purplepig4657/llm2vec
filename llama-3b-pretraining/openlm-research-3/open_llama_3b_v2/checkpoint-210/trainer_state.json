{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.11986622073578596,
  "eval_steps": 500,
  "global_step": 210,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005707915273132665,
      "grad_norm": 4.3125,
      "learning_rate": 2.9994288977727013e-05,
      "loss": 0.5774,
      "step": 1
    },
    {
      "epoch": 0.001141583054626533,
      "grad_norm": 1.03125,
      "learning_rate": 2.998857795545403e-05,
      "loss": 0.6062,
      "step": 2
    },
    {
      "epoch": 0.0017123745819397993,
      "grad_norm": 1.15625,
      "learning_rate": 2.998286693318104e-05,
      "loss": 0.5913,
      "step": 3
    },
    {
      "epoch": 0.002283166109253066,
      "grad_norm": 1.25,
      "learning_rate": 2.9977155910908053e-05,
      "loss": 0.51,
      "step": 4
    },
    {
      "epoch": 0.002853957636566332,
      "grad_norm": 1.2890625,
      "learning_rate": 2.9971444888635065e-05,
      "loss": 0.4734,
      "step": 5
    },
    {
      "epoch": 0.0034247491638795986,
      "grad_norm": 1.3125,
      "learning_rate": 2.996573386636208e-05,
      "loss": 0.4566,
      "step": 6
    },
    {
      "epoch": 0.003995540691192865,
      "grad_norm": 1.265625,
      "learning_rate": 2.9960022844089092e-05,
      "loss": 0.3998,
      "step": 7
    },
    {
      "epoch": 0.004566332218506132,
      "grad_norm": 1.34375,
      "learning_rate": 2.9954311821816108e-05,
      "loss": 0.3361,
      "step": 8
    },
    {
      "epoch": 0.005137123745819398,
      "grad_norm": 1.2890625,
      "learning_rate": 2.994860079954312e-05,
      "loss": 0.2833,
      "step": 9
    },
    {
      "epoch": 0.005707915273132664,
      "grad_norm": 1.390625,
      "learning_rate": 2.9942889777270132e-05,
      "loss": 0.2787,
      "step": 10
    },
    {
      "epoch": 0.006278706800445931,
      "grad_norm": 1.2578125,
      "learning_rate": 2.9937178754997144e-05,
      "loss": 0.2515,
      "step": 11
    },
    {
      "epoch": 0.006849498327759197,
      "grad_norm": 1.5,
      "learning_rate": 2.9931467732724156e-05,
      "loss": 0.2385,
      "step": 12
    },
    {
      "epoch": 0.007420289855072463,
      "grad_norm": 2.0,
      "learning_rate": 2.9925756710451172e-05,
      "loss": 0.2067,
      "step": 13
    },
    {
      "epoch": 0.00799108138238573,
      "grad_norm": 2.1875,
      "learning_rate": 2.9920045688178184e-05,
      "loss": 0.2204,
      "step": 14
    },
    {
      "epoch": 0.008561872909698997,
      "grad_norm": 1.203125,
      "learning_rate": 2.99143346659052e-05,
      "loss": 0.1557,
      "step": 15
    },
    {
      "epoch": 0.009132664437012264,
      "grad_norm": 1.1953125,
      "learning_rate": 2.990862364363221e-05,
      "loss": 0.1763,
      "step": 16
    },
    {
      "epoch": 0.009703455964325529,
      "grad_norm": 0.9921875,
      "learning_rate": 2.9902912621359224e-05,
      "loss": 0.1211,
      "step": 17
    },
    {
      "epoch": 0.010274247491638796,
      "grad_norm": 0.8828125,
      "learning_rate": 2.9897201599086236e-05,
      "loss": 0.1077,
      "step": 18
    },
    {
      "epoch": 0.010845039018952063,
      "grad_norm": 0.83984375,
      "learning_rate": 2.989149057681325e-05,
      "loss": 0.1172,
      "step": 19
    },
    {
      "epoch": 0.011415830546265328,
      "grad_norm": 0.75,
      "learning_rate": 2.9885779554540263e-05,
      "loss": 0.1087,
      "step": 20
    },
    {
      "epoch": 0.011986622073578595,
      "grad_norm": 0.75390625,
      "learning_rate": 2.9880068532267276e-05,
      "loss": 0.0914,
      "step": 21
    },
    {
      "epoch": 0.012557413600891862,
      "grad_norm": 0.578125,
      "learning_rate": 2.987435750999429e-05,
      "loss": 0.0737,
      "step": 22
    },
    {
      "epoch": 0.013128205128205127,
      "grad_norm": 0.91796875,
      "learning_rate": 2.9868646487721303e-05,
      "loss": 0.0747,
      "step": 23
    },
    {
      "epoch": 0.013698996655518395,
      "grad_norm": 0.71875,
      "learning_rate": 2.9862935465448315e-05,
      "loss": 0.0776,
      "step": 24
    },
    {
      "epoch": 0.014269788182831662,
      "grad_norm": 0.609375,
      "learning_rate": 2.9857224443175327e-05,
      "loss": 0.058,
      "step": 25
    },
    {
      "epoch": 0.014840579710144927,
      "grad_norm": 0.58984375,
      "learning_rate": 2.9851513420902343e-05,
      "loss": 0.0568,
      "step": 26
    },
    {
      "epoch": 0.015411371237458194,
      "grad_norm": 0.7109375,
      "learning_rate": 2.9845802398629355e-05,
      "loss": 0.0753,
      "step": 27
    },
    {
      "epoch": 0.01598216276477146,
      "grad_norm": 0.56640625,
      "learning_rate": 2.984009137635637e-05,
      "loss": 0.0467,
      "step": 28
    },
    {
      "epoch": 0.016552954292084726,
      "grad_norm": 0.49609375,
      "learning_rate": 2.9834380354083383e-05,
      "loss": 0.0459,
      "step": 29
    },
    {
      "epoch": 0.017123745819397993,
      "grad_norm": 0.45703125,
      "learning_rate": 2.9828669331810398e-05,
      "loss": 0.0587,
      "step": 30
    },
    {
      "epoch": 0.01769453734671126,
      "grad_norm": 0.41796875,
      "learning_rate": 2.9822958309537407e-05,
      "loss": 0.0408,
      "step": 31
    },
    {
      "epoch": 0.018265328874024527,
      "grad_norm": 0.4921875,
      "learning_rate": 2.981724728726442e-05,
      "loss": 0.0463,
      "step": 32
    },
    {
      "epoch": 0.018836120401337794,
      "grad_norm": 0.5625,
      "learning_rate": 2.9811536264991434e-05,
      "loss": 0.0345,
      "step": 33
    },
    {
      "epoch": 0.019406911928651058,
      "grad_norm": 0.482421875,
      "learning_rate": 2.9805825242718447e-05,
      "loss": 0.0616,
      "step": 34
    },
    {
      "epoch": 0.019977703455964325,
      "grad_norm": 0.33203125,
      "learning_rate": 2.9800114220445462e-05,
      "loss": 0.0383,
      "step": 35
    },
    {
      "epoch": 0.020548494983277592,
      "grad_norm": 0.34375,
      "learning_rate": 2.9794403198172474e-05,
      "loss": 0.0347,
      "step": 36
    },
    {
      "epoch": 0.02111928651059086,
      "grad_norm": 0.416015625,
      "learning_rate": 2.9788692175899486e-05,
      "loss": 0.0381,
      "step": 37
    },
    {
      "epoch": 0.021690078037904126,
      "grad_norm": 0.30078125,
      "learning_rate": 2.97829811536265e-05,
      "loss": 0.0377,
      "step": 38
    },
    {
      "epoch": 0.022260869565217393,
      "grad_norm": 0.439453125,
      "learning_rate": 2.9777270131353514e-05,
      "loss": 0.0423,
      "step": 39
    },
    {
      "epoch": 0.022831661092530656,
      "grad_norm": 0.328125,
      "learning_rate": 2.9771559109080526e-05,
      "loss": 0.0498,
      "step": 40
    },
    {
      "epoch": 0.023402452619843923,
      "grad_norm": 0.322265625,
      "learning_rate": 2.9765848086807538e-05,
      "loss": 0.0327,
      "step": 41
    },
    {
      "epoch": 0.02397324414715719,
      "grad_norm": 0.35546875,
      "learning_rate": 2.9760137064534554e-05,
      "loss": 0.0562,
      "step": 42
    },
    {
      "epoch": 0.024544035674470457,
      "grad_norm": 0.37109375,
      "learning_rate": 2.9754426042261566e-05,
      "loss": 0.0381,
      "step": 43
    },
    {
      "epoch": 0.025114827201783724,
      "grad_norm": 0.70703125,
      "learning_rate": 2.9748715019988578e-05,
      "loss": 0.0566,
      "step": 44
    },
    {
      "epoch": 0.02568561872909699,
      "grad_norm": 0.515625,
      "learning_rate": 2.974300399771559e-05,
      "loss": 0.0481,
      "step": 45
    },
    {
      "epoch": 0.026256410256410255,
      "grad_norm": 0.412109375,
      "learning_rate": 2.9737292975442606e-05,
      "loss": 0.0498,
      "step": 46
    },
    {
      "epoch": 0.026827201783723522,
      "grad_norm": 0.5234375,
      "learning_rate": 2.9731581953169618e-05,
      "loss": 0.0403,
      "step": 47
    },
    {
      "epoch": 0.02739799331103679,
      "grad_norm": 0.349609375,
      "learning_rate": 2.9725870930896633e-05,
      "loss": 0.0504,
      "step": 48
    },
    {
      "epoch": 0.027968784838350056,
      "grad_norm": 1.078125,
      "learning_rate": 2.9720159908623645e-05,
      "loss": 0.0453,
      "step": 49
    },
    {
      "epoch": 0.028539576365663323,
      "grad_norm": 0.3671875,
      "learning_rate": 2.971444888635066e-05,
      "loss": 0.0454,
      "step": 50
    },
    {
      "epoch": 0.02911036789297659,
      "grad_norm": 0.345703125,
      "learning_rate": 2.970873786407767e-05,
      "loss": 0.0393,
      "step": 51
    },
    {
      "epoch": 0.029681159420289854,
      "grad_norm": 0.3046875,
      "learning_rate": 2.970302684180468e-05,
      "loss": 0.0349,
      "step": 52
    },
    {
      "epoch": 0.03025195094760312,
      "grad_norm": 0.400390625,
      "learning_rate": 2.9697315819531697e-05,
      "loss": 0.0356,
      "step": 53
    },
    {
      "epoch": 0.030822742474916388,
      "grad_norm": 0.32421875,
      "learning_rate": 2.969160479725871e-05,
      "loss": 0.0341,
      "step": 54
    },
    {
      "epoch": 0.031393534002229655,
      "grad_norm": 0.216796875,
      "learning_rate": 2.9685893774985725e-05,
      "loss": 0.0222,
      "step": 55
    },
    {
      "epoch": 0.03196432552954292,
      "grad_norm": 0.298828125,
      "learning_rate": 2.9680182752712737e-05,
      "loss": 0.0353,
      "step": 56
    },
    {
      "epoch": 0.03253511705685619,
      "grad_norm": 0.24609375,
      "learning_rate": 2.967447173043975e-05,
      "loss": 0.0277,
      "step": 57
    },
    {
      "epoch": 0.03310590858416945,
      "grad_norm": 0.353515625,
      "learning_rate": 2.966876070816676e-05,
      "loss": 0.0358,
      "step": 58
    },
    {
      "epoch": 0.03367670011148272,
      "grad_norm": 0.33203125,
      "learning_rate": 2.9663049685893777e-05,
      "loss": 0.0316,
      "step": 59
    },
    {
      "epoch": 0.034247491638795986,
      "grad_norm": 0.345703125,
      "learning_rate": 2.965733866362079e-05,
      "loss": 0.0285,
      "step": 60
    },
    {
      "epoch": 0.03481828316610925,
      "grad_norm": 0.439453125,
      "learning_rate": 2.96516276413478e-05,
      "loss": 0.0301,
      "step": 61
    },
    {
      "epoch": 0.03538907469342252,
      "grad_norm": 0.26171875,
      "learning_rate": 2.9645916619074816e-05,
      "loss": 0.0282,
      "step": 62
    },
    {
      "epoch": 0.035959866220735784,
      "grad_norm": 0.330078125,
      "learning_rate": 2.964020559680183e-05,
      "loss": 0.0413,
      "step": 63
    },
    {
      "epoch": 0.036530657748049054,
      "grad_norm": 0.5390625,
      "learning_rate": 2.963449457452884e-05,
      "loss": 0.0398,
      "step": 64
    },
    {
      "epoch": 0.03710144927536232,
      "grad_norm": 0.30859375,
      "learning_rate": 2.9628783552255853e-05,
      "loss": 0.0343,
      "step": 65
    },
    {
      "epoch": 0.03767224080267559,
      "grad_norm": 0.376953125,
      "learning_rate": 2.9623072529982868e-05,
      "loss": 0.0354,
      "step": 66
    },
    {
      "epoch": 0.03824303232998885,
      "grad_norm": 0.341796875,
      "learning_rate": 2.961736150770988e-05,
      "loss": 0.0536,
      "step": 67
    },
    {
      "epoch": 0.038813823857302115,
      "grad_norm": 0.35546875,
      "learning_rate": 2.9611650485436896e-05,
      "loss": 0.0234,
      "step": 68
    },
    {
      "epoch": 0.039384615384615386,
      "grad_norm": 0.259765625,
      "learning_rate": 2.9605939463163908e-05,
      "loss": 0.0421,
      "step": 69
    },
    {
      "epoch": 0.03995540691192865,
      "grad_norm": 0.30859375,
      "learning_rate": 2.9600228440890923e-05,
      "loss": 0.0265,
      "step": 70
    },
    {
      "epoch": 0.04052619843924192,
      "grad_norm": 0.251953125,
      "learning_rate": 2.9594517418617932e-05,
      "loss": 0.0274,
      "step": 71
    },
    {
      "epoch": 0.041096989966555184,
      "grad_norm": 0.298828125,
      "learning_rate": 2.9588806396344944e-05,
      "loss": 0.0349,
      "step": 72
    },
    {
      "epoch": 0.04166778149386845,
      "grad_norm": 0.53125,
      "learning_rate": 2.958309537407196e-05,
      "loss": 0.0383,
      "step": 73
    },
    {
      "epoch": 0.04223857302118172,
      "grad_norm": 0.310546875,
      "learning_rate": 2.9577384351798972e-05,
      "loss": 0.0303,
      "step": 74
    },
    {
      "epoch": 0.04280936454849498,
      "grad_norm": 0.302734375,
      "learning_rate": 2.9571673329525987e-05,
      "loss": 0.0244,
      "step": 75
    },
    {
      "epoch": 0.04338015607580825,
      "grad_norm": 0.76171875,
      "learning_rate": 2.9565962307253e-05,
      "loss": 0.0216,
      "step": 76
    },
    {
      "epoch": 0.043950947603121515,
      "grad_norm": 0.36328125,
      "learning_rate": 2.956025128498001e-05,
      "loss": 0.0323,
      "step": 77
    },
    {
      "epoch": 0.044521739130434786,
      "grad_norm": 0.546875,
      "learning_rate": 2.9554540262707024e-05,
      "loss": 0.0385,
      "step": 78
    },
    {
      "epoch": 0.04509253065774805,
      "grad_norm": 0.380859375,
      "learning_rate": 2.954882924043404e-05,
      "loss": 0.0337,
      "step": 79
    },
    {
      "epoch": 0.04566332218506131,
      "grad_norm": 0.318359375,
      "learning_rate": 2.954311821816105e-05,
      "loss": 0.0239,
      "step": 80
    },
    {
      "epoch": 0.04623411371237458,
      "grad_norm": 0.48046875,
      "learning_rate": 2.9537407195888063e-05,
      "loss": 0.0461,
      "step": 81
    },
    {
      "epoch": 0.04680490523968785,
      "grad_norm": 0.38671875,
      "learning_rate": 2.953169617361508e-05,
      "loss": 0.0375,
      "step": 82
    },
    {
      "epoch": 0.04737569676700112,
      "grad_norm": 0.84375,
      "learning_rate": 2.952598515134209e-05,
      "loss": 0.0303,
      "step": 83
    },
    {
      "epoch": 0.04794648829431438,
      "grad_norm": 0.2392578125,
      "learning_rate": 2.9520274129069103e-05,
      "loss": 0.0323,
      "step": 84
    },
    {
      "epoch": 0.048517279821627644,
      "grad_norm": 0.216796875,
      "learning_rate": 2.9514563106796115e-05,
      "loss": 0.0204,
      "step": 85
    },
    {
      "epoch": 0.049088071348940915,
      "grad_norm": 0.291015625,
      "learning_rate": 2.950885208452313e-05,
      "loss": 0.023,
      "step": 86
    },
    {
      "epoch": 0.04965886287625418,
      "grad_norm": 0.53125,
      "learning_rate": 2.9503141062250143e-05,
      "loss": 0.0334,
      "step": 87
    },
    {
      "epoch": 0.05022965440356745,
      "grad_norm": 0.353515625,
      "learning_rate": 2.949743003997716e-05,
      "loss": 0.0263,
      "step": 88
    },
    {
      "epoch": 0.05080044593088071,
      "grad_norm": 0.369140625,
      "learning_rate": 2.949171901770417e-05,
      "loss": 0.0398,
      "step": 89
    },
    {
      "epoch": 0.05137123745819398,
      "grad_norm": 0.236328125,
      "learning_rate": 2.9486007995431186e-05,
      "loss": 0.0184,
      "step": 90
    },
    {
      "epoch": 0.051942028985507246,
      "grad_norm": 0.33203125,
      "learning_rate": 2.9480296973158195e-05,
      "loss": 0.022,
      "step": 91
    },
    {
      "epoch": 0.05251282051282051,
      "grad_norm": 0.349609375,
      "learning_rate": 2.9474585950885207e-05,
      "loss": 0.0342,
      "step": 92
    },
    {
      "epoch": 0.05308361204013378,
      "grad_norm": 0.34375,
      "learning_rate": 2.9468874928612222e-05,
      "loss": 0.0266,
      "step": 93
    },
    {
      "epoch": 0.053654403567447044,
      "grad_norm": 0.2314453125,
      "learning_rate": 2.9463163906339235e-05,
      "loss": 0.0169,
      "step": 94
    },
    {
      "epoch": 0.054225195094760315,
      "grad_norm": 0.2197265625,
      "learning_rate": 2.945745288406625e-05,
      "loss": 0.0192,
      "step": 95
    },
    {
      "epoch": 0.05479598662207358,
      "grad_norm": 0.431640625,
      "learning_rate": 2.9451741861793262e-05,
      "loss": 0.0251,
      "step": 96
    },
    {
      "epoch": 0.05536677814938684,
      "grad_norm": 0.35546875,
      "learning_rate": 2.9446030839520274e-05,
      "loss": 0.0256,
      "step": 97
    },
    {
      "epoch": 0.05593756967670011,
      "grad_norm": 0.515625,
      "learning_rate": 2.9440319817247286e-05,
      "loss": 0.0265,
      "step": 98
    },
    {
      "epoch": 0.056508361204013376,
      "grad_norm": 0.322265625,
      "learning_rate": 2.9434608794974302e-05,
      "loss": 0.0217,
      "step": 99
    },
    {
      "epoch": 0.057079152731326646,
      "grad_norm": 0.302734375,
      "learning_rate": 2.9428897772701314e-05,
      "loss": 0.0217,
      "step": 100
    },
    {
      "epoch": 0.05764994425863991,
      "grad_norm": 0.279296875,
      "learning_rate": 2.9423186750428326e-05,
      "loss": 0.0153,
      "step": 101
    },
    {
      "epoch": 0.05822073578595318,
      "grad_norm": 0.3515625,
      "learning_rate": 2.941747572815534e-05,
      "loss": 0.025,
      "step": 102
    },
    {
      "epoch": 0.058791527313266444,
      "grad_norm": 0.32421875,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 0.0238,
      "step": 103
    },
    {
      "epoch": 0.05936231884057971,
      "grad_norm": 0.197265625,
      "learning_rate": 2.9406053683609366e-05,
      "loss": 0.0156,
      "step": 104
    },
    {
      "epoch": 0.05993311036789298,
      "grad_norm": 0.330078125,
      "learning_rate": 2.9400342661336378e-05,
      "loss": 0.0214,
      "step": 105
    },
    {
      "epoch": 0.06050390189520624,
      "grad_norm": 0.208984375,
      "learning_rate": 2.9394631639063393e-05,
      "loss": 0.0218,
      "step": 106
    },
    {
      "epoch": 0.06107469342251951,
      "grad_norm": 0.2080078125,
      "learning_rate": 2.9388920616790406e-05,
      "loss": 0.0151,
      "step": 107
    },
    {
      "epoch": 0.061645484949832775,
      "grad_norm": 0.298828125,
      "learning_rate": 2.938320959451742e-05,
      "loss": 0.0281,
      "step": 108
    },
    {
      "epoch": 0.062216276477146046,
      "grad_norm": 0.302734375,
      "learning_rate": 2.9377498572244433e-05,
      "loss": 0.0177,
      "step": 109
    },
    {
      "epoch": 0.06278706800445931,
      "grad_norm": 0.28515625,
      "learning_rate": 2.937178754997145e-05,
      "loss": 0.0202,
      "step": 110
    },
    {
      "epoch": 0.06335785953177257,
      "grad_norm": 0.224609375,
      "learning_rate": 2.9366076527698457e-05,
      "loss": 0.0171,
      "step": 111
    },
    {
      "epoch": 0.06392865105908584,
      "grad_norm": 0.251953125,
      "learning_rate": 2.936036550542547e-05,
      "loss": 0.0236,
      "step": 112
    },
    {
      "epoch": 0.06449944258639911,
      "grad_norm": 0.26953125,
      "learning_rate": 2.9354654483152485e-05,
      "loss": 0.0255,
      "step": 113
    },
    {
      "epoch": 0.06507023411371238,
      "grad_norm": 0.27734375,
      "learning_rate": 2.9348943460879497e-05,
      "loss": 0.0232,
      "step": 114
    },
    {
      "epoch": 0.06564102564102564,
      "grad_norm": 0.22265625,
      "learning_rate": 2.9343232438606513e-05,
      "loss": 0.0163,
      "step": 115
    },
    {
      "epoch": 0.0662118171683389,
      "grad_norm": 0.1689453125,
      "learning_rate": 2.9337521416333525e-05,
      "loss": 0.0106,
      "step": 116
    },
    {
      "epoch": 0.06678260869565217,
      "grad_norm": 0.369140625,
      "learning_rate": 2.933181039406054e-05,
      "loss": 0.0167,
      "step": 117
    },
    {
      "epoch": 0.06735340022296545,
      "grad_norm": 0.244140625,
      "learning_rate": 2.932609937178755e-05,
      "loss": 0.0156,
      "step": 118
    },
    {
      "epoch": 0.06792419175027871,
      "grad_norm": 0.1748046875,
      "learning_rate": 2.9320388349514565e-05,
      "loss": 0.009,
      "step": 119
    },
    {
      "epoch": 0.06849498327759197,
      "grad_norm": 0.248046875,
      "learning_rate": 2.9314677327241577e-05,
      "loss": 0.0211,
      "step": 120
    },
    {
      "epoch": 0.06906577480490524,
      "grad_norm": 0.23828125,
      "learning_rate": 2.930896630496859e-05,
      "loss": 0.0119,
      "step": 121
    },
    {
      "epoch": 0.0696365663322185,
      "grad_norm": 0.26953125,
      "learning_rate": 2.9303255282695604e-05,
      "loss": 0.0151,
      "step": 122
    },
    {
      "epoch": 0.07020735785953178,
      "grad_norm": 0.1689453125,
      "learning_rate": 2.9297544260422616e-05,
      "loss": 0.0077,
      "step": 123
    },
    {
      "epoch": 0.07077814938684504,
      "grad_norm": 0.3203125,
      "learning_rate": 2.929183323814963e-05,
      "loss": 0.024,
      "step": 124
    },
    {
      "epoch": 0.0713489409141583,
      "grad_norm": 0.2412109375,
      "learning_rate": 2.928612221587664e-05,
      "loss": 0.0122,
      "step": 125
    },
    {
      "epoch": 0.07191973244147157,
      "grad_norm": 0.263671875,
      "learning_rate": 2.9280411193603656e-05,
      "loss": 0.0186,
      "step": 126
    },
    {
      "epoch": 0.07249052396878485,
      "grad_norm": 0.150390625,
      "learning_rate": 2.9274700171330668e-05,
      "loss": 0.0086,
      "step": 127
    },
    {
      "epoch": 0.07306131549609811,
      "grad_norm": 0.2197265625,
      "learning_rate": 2.9268989149057684e-05,
      "loss": 0.0168,
      "step": 128
    },
    {
      "epoch": 0.07363210702341137,
      "grad_norm": 0.224609375,
      "learning_rate": 2.9263278126784696e-05,
      "loss": 0.0121,
      "step": 129
    },
    {
      "epoch": 0.07420289855072464,
      "grad_norm": 0.314453125,
      "learning_rate": 2.925756710451171e-05,
      "loss": 0.0243,
      "step": 130
    },
    {
      "epoch": 0.0747736900780379,
      "grad_norm": 0.318359375,
      "learning_rate": 2.925185608223872e-05,
      "loss": 0.018,
      "step": 131
    },
    {
      "epoch": 0.07534448160535118,
      "grad_norm": 0.2392578125,
      "learning_rate": 2.9246145059965732e-05,
      "loss": 0.015,
      "step": 132
    },
    {
      "epoch": 0.07591527313266444,
      "grad_norm": 0.255859375,
      "learning_rate": 2.9240434037692748e-05,
      "loss": 0.0207,
      "step": 133
    },
    {
      "epoch": 0.0764860646599777,
      "grad_norm": 0.28125,
      "learning_rate": 2.923472301541976e-05,
      "loss": 0.0142,
      "step": 134
    },
    {
      "epoch": 0.07705685618729097,
      "grad_norm": 0.283203125,
      "learning_rate": 2.9229011993146775e-05,
      "loss": 0.0135,
      "step": 135
    },
    {
      "epoch": 0.07762764771460423,
      "grad_norm": 0.208984375,
      "learning_rate": 2.9223300970873787e-05,
      "loss": 0.0102,
      "step": 136
    },
    {
      "epoch": 0.07819843924191751,
      "grad_norm": 0.1904296875,
      "learning_rate": 2.9217589948600803e-05,
      "loss": 0.0152,
      "step": 137
    },
    {
      "epoch": 0.07876923076923077,
      "grad_norm": 0.2890625,
      "learning_rate": 2.9211878926327812e-05,
      "loss": 0.0184,
      "step": 138
    },
    {
      "epoch": 0.07934002229654404,
      "grad_norm": 0.1953125,
      "learning_rate": 2.9206167904054827e-05,
      "loss": 0.0113,
      "step": 139
    },
    {
      "epoch": 0.0799108138238573,
      "grad_norm": 0.2197265625,
      "learning_rate": 2.920045688178184e-05,
      "loss": 0.011,
      "step": 140
    },
    {
      "epoch": 0.08048160535117056,
      "grad_norm": 0.1875,
      "learning_rate": 2.919474585950885e-05,
      "loss": 0.0111,
      "step": 141
    },
    {
      "epoch": 0.08105239687848384,
      "grad_norm": 0.21484375,
      "learning_rate": 2.9189034837235867e-05,
      "loss": 0.0133,
      "step": 142
    },
    {
      "epoch": 0.0816231884057971,
      "grad_norm": 0.337890625,
      "learning_rate": 2.918332381496288e-05,
      "loss": 0.0096,
      "step": 143
    },
    {
      "epoch": 0.08219397993311037,
      "grad_norm": 0.23046875,
      "learning_rate": 2.917761279268989e-05,
      "loss": 0.017,
      "step": 144
    },
    {
      "epoch": 0.08276477146042363,
      "grad_norm": 0.3046875,
      "learning_rate": 2.9171901770416903e-05,
      "loss": 0.0156,
      "step": 145
    },
    {
      "epoch": 0.0833355629877369,
      "grad_norm": 0.1865234375,
      "learning_rate": 2.916619074814392e-05,
      "loss": 0.0103,
      "step": 146
    },
    {
      "epoch": 0.08390635451505017,
      "grad_norm": 0.25390625,
      "learning_rate": 2.916047972587093e-05,
      "loss": 0.0144,
      "step": 147
    },
    {
      "epoch": 0.08447714604236344,
      "grad_norm": 0.2421875,
      "learning_rate": 2.9154768703597946e-05,
      "loss": 0.0153,
      "step": 148
    },
    {
      "epoch": 0.0850479375696767,
      "grad_norm": 0.2001953125,
      "learning_rate": 2.914905768132496e-05,
      "loss": 0.0116,
      "step": 149
    },
    {
      "epoch": 0.08561872909698996,
      "grad_norm": 0.267578125,
      "learning_rate": 2.9143346659051974e-05,
      "loss": 0.0124,
      "step": 150
    },
    {
      "epoch": 0.08618952062430324,
      "grad_norm": 0.255859375,
      "learning_rate": 2.9137635636778983e-05,
      "loss": 0.0119,
      "step": 151
    },
    {
      "epoch": 0.0867603121516165,
      "grad_norm": 0.1669921875,
      "learning_rate": 2.9131924614505995e-05,
      "loss": 0.0074,
      "step": 152
    },
    {
      "epoch": 0.08733110367892977,
      "grad_norm": 0.2001953125,
      "learning_rate": 2.912621359223301e-05,
      "loss": 0.0117,
      "step": 153
    },
    {
      "epoch": 0.08790189520624303,
      "grad_norm": 0.216796875,
      "learning_rate": 2.9120502569960022e-05,
      "loss": 0.0111,
      "step": 154
    },
    {
      "epoch": 0.0884726867335563,
      "grad_norm": 0.24609375,
      "learning_rate": 2.9114791547687038e-05,
      "loss": 0.0137,
      "step": 155
    },
    {
      "epoch": 0.08904347826086957,
      "grad_norm": 0.234375,
      "learning_rate": 2.910908052541405e-05,
      "loss": 0.0118,
      "step": 156
    },
    {
      "epoch": 0.08961426978818283,
      "grad_norm": 0.2314453125,
      "learning_rate": 2.9103369503141066e-05,
      "loss": 0.0139,
      "step": 157
    },
    {
      "epoch": 0.0901850613154961,
      "grad_norm": 0.30859375,
      "learning_rate": 2.9097658480868074e-05,
      "loss": 0.0138,
      "step": 158
    },
    {
      "epoch": 0.09075585284280936,
      "grad_norm": 0.28125,
      "learning_rate": 2.909194745859509e-05,
      "loss": 0.0142,
      "step": 159
    },
    {
      "epoch": 0.09132664437012263,
      "grad_norm": 0.228515625,
      "learning_rate": 2.9086236436322102e-05,
      "loss": 0.0151,
      "step": 160
    },
    {
      "epoch": 0.0918974358974359,
      "grad_norm": 0.265625,
      "learning_rate": 2.9080525414049114e-05,
      "loss": 0.0127,
      "step": 161
    },
    {
      "epoch": 0.09246822742474917,
      "grad_norm": 0.37109375,
      "learning_rate": 2.907481439177613e-05,
      "loss": 0.0114,
      "step": 162
    },
    {
      "epoch": 0.09303901895206243,
      "grad_norm": 0.2041015625,
      "learning_rate": 2.9069103369503142e-05,
      "loss": 0.011,
      "step": 163
    },
    {
      "epoch": 0.0936098104793757,
      "grad_norm": 0.166015625,
      "learning_rate": 2.9063392347230154e-05,
      "loss": 0.0077,
      "step": 164
    },
    {
      "epoch": 0.09418060200668896,
      "grad_norm": 0.15625,
      "learning_rate": 2.9057681324957166e-05,
      "loss": 0.0089,
      "step": 165
    },
    {
      "epoch": 0.09475139353400223,
      "grad_norm": 0.26171875,
      "learning_rate": 2.905197030268418e-05,
      "loss": 0.0125,
      "step": 166
    },
    {
      "epoch": 0.0953221850613155,
      "grad_norm": 0.60546875,
      "learning_rate": 2.9046259280411194e-05,
      "loss": 0.0093,
      "step": 167
    },
    {
      "epoch": 0.09589297658862876,
      "grad_norm": 0.1728515625,
      "learning_rate": 2.904054825813821e-05,
      "loss": 0.0077,
      "step": 168
    },
    {
      "epoch": 0.09646376811594203,
      "grad_norm": 0.35546875,
      "learning_rate": 2.903483723586522e-05,
      "loss": 0.0167,
      "step": 169
    },
    {
      "epoch": 0.09703455964325529,
      "grad_norm": 0.1767578125,
      "learning_rate": 2.9029126213592237e-05,
      "loss": 0.0074,
      "step": 170
    },
    {
      "epoch": 0.09760535117056857,
      "grad_norm": 0.193359375,
      "learning_rate": 2.9023415191319245e-05,
      "loss": 0.0096,
      "step": 171
    },
    {
      "epoch": 0.09817614269788183,
      "grad_norm": 0.1953125,
      "learning_rate": 2.9017704169046258e-05,
      "loss": 0.0118,
      "step": 172
    },
    {
      "epoch": 0.0987469342251951,
      "grad_norm": 0.259765625,
      "learning_rate": 2.9011993146773273e-05,
      "loss": 0.0159,
      "step": 173
    },
    {
      "epoch": 0.09931772575250836,
      "grad_norm": 0.177734375,
      "learning_rate": 2.9006282124500285e-05,
      "loss": 0.0091,
      "step": 174
    },
    {
      "epoch": 0.09988851727982163,
      "grad_norm": 0.228515625,
      "learning_rate": 2.90005711022273e-05,
      "loss": 0.0082,
      "step": 175
    },
    {
      "epoch": 0.1004593088071349,
      "grad_norm": 0.201171875,
      "learning_rate": 2.8994860079954313e-05,
      "loss": 0.0097,
      "step": 176
    },
    {
      "epoch": 0.10103010033444816,
      "grad_norm": 0.58984375,
      "learning_rate": 2.8989149057681328e-05,
      "loss": 0.0168,
      "step": 177
    },
    {
      "epoch": 0.10160089186176142,
      "grad_norm": 0.2236328125,
      "learning_rate": 2.8983438035408337e-05,
      "loss": 0.0135,
      "step": 178
    },
    {
      "epoch": 0.10217168338907469,
      "grad_norm": 0.2275390625,
      "learning_rate": 2.8977727013135353e-05,
      "loss": 0.012,
      "step": 179
    },
    {
      "epoch": 0.10274247491638797,
      "grad_norm": 0.302734375,
      "learning_rate": 2.8972015990862365e-05,
      "loss": 0.0174,
      "step": 180
    },
    {
      "epoch": 0.10331326644370123,
      "grad_norm": 0.169921875,
      "learning_rate": 2.8966304968589377e-05,
      "loss": 0.0096,
      "step": 181
    },
    {
      "epoch": 0.10388405797101449,
      "grad_norm": 0.24609375,
      "learning_rate": 2.8960593946316392e-05,
      "loss": 0.0154,
      "step": 182
    },
    {
      "epoch": 0.10445484949832776,
      "grad_norm": 0.19921875,
      "learning_rate": 2.8954882924043404e-05,
      "loss": 0.0101,
      "step": 183
    },
    {
      "epoch": 0.10502564102564102,
      "grad_norm": 0.2197265625,
      "learning_rate": 2.894917190177042e-05,
      "loss": 0.0144,
      "step": 184
    },
    {
      "epoch": 0.1055964325529543,
      "grad_norm": 0.27734375,
      "learning_rate": 2.894346087949743e-05,
      "loss": 0.0174,
      "step": 185
    },
    {
      "epoch": 0.10616722408026756,
      "grad_norm": 0.201171875,
      "learning_rate": 2.8937749857224444e-05,
      "loss": 0.0084,
      "step": 186
    },
    {
      "epoch": 0.10673801560758082,
      "grad_norm": 0.1806640625,
      "learning_rate": 2.8932038834951456e-05,
      "loss": 0.0077,
      "step": 187
    },
    {
      "epoch": 0.10730880713489409,
      "grad_norm": 0.2216796875,
      "learning_rate": 2.8926327812678472e-05,
      "loss": 0.0118,
      "step": 188
    },
    {
      "epoch": 0.10787959866220735,
      "grad_norm": 0.412109375,
      "learning_rate": 2.8920616790405484e-05,
      "loss": 0.0212,
      "step": 189
    },
    {
      "epoch": 0.10845039018952063,
      "grad_norm": 0.171875,
      "learning_rate": 2.89149057681325e-05,
      "loss": 0.0056,
      "step": 190
    },
    {
      "epoch": 0.10902118171683389,
      "grad_norm": 0.251953125,
      "learning_rate": 2.8909194745859508e-05,
      "loss": 0.01,
      "step": 191
    },
    {
      "epoch": 0.10959197324414716,
      "grad_norm": 0.1962890625,
      "learning_rate": 2.890348372358652e-05,
      "loss": 0.0085,
      "step": 192
    },
    {
      "epoch": 0.11016276477146042,
      "grad_norm": 0.1748046875,
      "learning_rate": 2.8897772701313536e-05,
      "loss": 0.009,
      "step": 193
    },
    {
      "epoch": 0.11073355629877368,
      "grad_norm": 0.2158203125,
      "learning_rate": 2.8892061679040548e-05,
      "loss": 0.0111,
      "step": 194
    },
    {
      "epoch": 0.11130434782608696,
      "grad_norm": 0.189453125,
      "learning_rate": 2.8886350656767563e-05,
      "loss": 0.0071,
      "step": 195
    },
    {
      "epoch": 0.11187513935340022,
      "grad_norm": 0.1650390625,
      "learning_rate": 2.8880639634494575e-05,
      "loss": 0.0075,
      "step": 196
    },
    {
      "epoch": 0.11244593088071349,
      "grad_norm": 0.1474609375,
      "learning_rate": 2.887492861222159e-05,
      "loss": 0.0119,
      "step": 197
    },
    {
      "epoch": 0.11301672240802675,
      "grad_norm": 0.1728515625,
      "learning_rate": 2.88692175899486e-05,
      "loss": 0.0078,
      "step": 198
    },
    {
      "epoch": 0.11358751393534003,
      "grad_norm": 0.1142578125,
      "learning_rate": 2.8863506567675615e-05,
      "loss": 0.0061,
      "step": 199
    },
    {
      "epoch": 0.11415830546265329,
      "grad_norm": 0.1787109375,
      "learning_rate": 2.8857795545402627e-05,
      "loss": 0.0105,
      "step": 200
    },
    {
      "epoch": 0.11472909698996656,
      "grad_norm": 0.2080078125,
      "learning_rate": 2.885208452312964e-05,
      "loss": 0.0111,
      "step": 201
    },
    {
      "epoch": 0.11529988851727982,
      "grad_norm": 0.216796875,
      "learning_rate": 2.8846373500856655e-05,
      "loss": 0.0086,
      "step": 202
    },
    {
      "epoch": 0.11587068004459308,
      "grad_norm": 0.1513671875,
      "learning_rate": 2.8840662478583667e-05,
      "loss": 0.0065,
      "step": 203
    },
    {
      "epoch": 0.11644147157190636,
      "grad_norm": 0.2333984375,
      "learning_rate": 2.8834951456310683e-05,
      "loss": 0.01,
      "step": 204
    },
    {
      "epoch": 0.11701226309921962,
      "grad_norm": 0.201171875,
      "learning_rate": 2.882924043403769e-05,
      "loss": 0.0093,
      "step": 205
    },
    {
      "epoch": 0.11758305462653289,
      "grad_norm": 0.2431640625,
      "learning_rate": 2.8823529411764707e-05,
      "loss": 0.0105,
      "step": 206
    },
    {
      "epoch": 0.11815384615384615,
      "grad_norm": 0.2119140625,
      "learning_rate": 2.881781838949172e-05,
      "loss": 0.0062,
      "step": 207
    },
    {
      "epoch": 0.11872463768115941,
      "grad_norm": 0.1826171875,
      "learning_rate": 2.8812107367218734e-05,
      "loss": 0.0082,
      "step": 208
    },
    {
      "epoch": 0.11929542920847269,
      "grad_norm": 0.1708984375,
      "learning_rate": 2.8806396344945746e-05,
      "loss": 0.0083,
      "step": 209
    },
    {
      "epoch": 0.11986622073578596,
      "grad_norm": 0.185546875,
      "learning_rate": 2.8800685322672762e-05,
      "loss": 0.0109,
      "step": 210
    }
  ],
  "logging_steps": 1,
  "max_steps": 5253,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
